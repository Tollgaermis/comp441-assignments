{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Convolutional Neural Networks\n",
    "Instructions: In Assignment 2, you will learn all about the convolutional neural networks. In particular, you will gain a first-hand experience of the training process, understand the architectural details, and familiarize with transfer learning\n",
    "with deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Convolutional Neural Networks\n",
    "In this part, you will experiment with a convolutional neural network implementation to perform image classification. The dataset we will use for this assignment was created by Zoya Bylinskii, and contains 451 works of art from 11 different artists all downsampled and padded to the same size. The task is to identify which artist produced each image. The original images can be found in the `art_data/artists` directory included with the data zip file. The composition of the dataset and a sample painting from each artist are shown in Table 1.\n",
    "\n",
    "Figure 1 shows an example of the type of convolutional architecture typically employed for similar image recognition problems. Convolutional layers apply filters to the image, and produce layers of\n",
    "feature maps. Often, the convolutional layers are interspersed with pooling layers. The final layers of the network are fully connected, and lead to an output layer with one node for each of the K classes\n",
    "the network is trying to detect. We will use a similar architecture for our network.\n",
    "\n",
    "![](figures/figure1.jpg)\n",
    "\n",
    "The code for performing the data processing and training the network is provided in the starter\n",
    "pack. You will use PyTorch to implement convolutional neural networks. We create a dataset from the artists’ images by downsampling them to 50x50 pixels, and transforming the RGB values to lie within the range $[-0.5, 0.5]$. We provide a lot of starter code below, but you will need to modify the hyperparameters and network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: Convolutional Filter Receptive Field\n",
    "\n",
    "First, it is important to develop an intuition for how a convolutional layer affects the feature representations that the network learns. Assume that you have a network in which the first convolutional layer\n",
    "applies a 5x5 patch to the image, producing a feature map $Z_{1}$. The next layer of the network is also convolutional; in this case, a 3x3 patch is applied to the feature map $Z_{1}$ to produce a new feature\n",
    "map, $Z_{2}$. Assume the stride used in both cases is 1. Let the receptive field of a node in this network be the portion of the original image that contributes information to the node (that it can, through the filters of the network, “see”). What are the dimensions of the receptive field for a node in $Z_{2}$? Note that you can ignore padding, and just consider patches in the middle of the image and $Z_{1}$. Thinking about your answer, why is it effective to build convolutional networks deeper, i.e. with more layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**<br>\n",
    "For a node in $Z_{2}$ the receptive field is 7x7. A 7x7 receptive field can be accomplished directly by using a 7x7 filter. But if we do it as explained above or even better with 3 3x3 filters, we will use more non-linear layers and the extracted features will be improved.<br> Also, using a single 7x7 layer, we will need K x (7 x 7 x C) = 49 x K x C parameters where K is the number of filters (or output channels) and C is the number of input channels. Whereas using 3 3x3 filters we will need  3 x (K x (3 x 3 x C)) = 27 x K x C for the same output volume, less that a single convoluional layer with a 7x7 filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Run the PyTorch ConvNet\n",
    "\n",
    "Study the provided SimpleCNN class below, and take a look at the hyperparameters. Answer the following questions about the initial implementation:\n",
    "\n",
    "1) How many layers are there? Are they all convolutional? If not, what structure do they have?\n",
    "2) Which activation function is used on the hidden nodes?\n",
    "3) What loss function is being used to train the network?\n",
    "4) How is the loss being minimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**<br>\n",
    "The architecture:<br>\n",
    "CONV -> ReLU -> POOL (?) -> CONV -> RelLU -> ReLU -> POOL (?) -> FC -> ReLU -> FC<br>\n",
    "There are 2 convolutional layers, which are both followed by ReLU activation functions. Depending on the pooling flag, these are followed by pooling. At the end, there are 2 fully connected layers and a ReLU activation function in between.<br>\n",
    "Cross entropy loss is being used in this multiclass classification problem.<br>\n",
    "The loss is being minimized by backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are familiar with the code, try training the network. It should take between 60-120 seconds to train for 50 epochs. What is the training accuracy for your network after training? What is the validation accuracy? What do these two numbers tell you about what your network is doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageFile\n",
    "import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import time\n",
    "import random\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import optim\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleCNN(torch.nn.Module):\n",
    "    def __init__(self, device, num_conv_layers=2, in_channels=3, conv_out_channels=[16, 16],\n",
    "                 kernel_sizes=[5, 5], strides=[2, 2], pooling=False, dropout_flag=False):\n",
    "        super(FlexibleCNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.pooling = pooling\n",
    "        self.dropout_flag = dropout_flag\n",
    "        \n",
    "        # Define convolutional layers based on specified parameters\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        current_in_channels = in_channels\n",
    "        for i in range(num_conv_layers):\n",
    "            conv_layer = torch.nn.Conv2d(in_channels=current_in_channels,\n",
    "                                         out_channels=conv_out_channels[i],\n",
    "                                         kernel_size=kernel_sizes[i],\n",
    "                                         stride=strides[i],\n",
    "                                         device=device)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            current_in_channels = conv_out_channels[i]\n",
    "        \n",
    "        # Define pooling layers if pooling is enabled\n",
    "        if pooling:\n",
    "            self.pool_layers = nn.ModuleList([torch.nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "                                              for _ in range(num_conv_layers)])\n",
    "        \n",
    "        # Define fully connected layers based on the final convolutional output size\n",
    "        self.fc_input_size = (self.compute_fc_input_size(kernel_sizes, strides)**2) * conv_out_channels[-1]\n",
    "        #print(f'fc_input_size: {self.fc_input_size}')\n",
    "        self.fully_connected_layer = nn.Linear(self.fc_input_size, 64, device=device)\n",
    "        self.final_layer = nn.Linear(64, 11, device=device)\n",
    "        \n",
    "        # Optional dropout layer\n",
    "        if dropout_flag:\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def compute_fc_input_size(self, kernel_sizes, strides):\n",
    "        w_in = 50\n",
    "        for f, s in zip(kernel_sizes, strides):\n",
    "            w_in = math.floor((w_in - f)/s) +1\n",
    "            if self.pooling:\n",
    "                # filter size and stride both equal to 2 by defaul for pooling\n",
    "                w_in = math.floor((w_in - 2) / 2) + 1\n",
    "        #print(f'calculated input to fc: {w_in}x{w_in}x16')\n",
    "        return w_in\n",
    "\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            x = torch.nn.functional.relu(conv_layer(x))\n",
    "            if self.pooling:\n",
    "                x = self.pool_layers[i](x)  # Apply corresponding pooling layer if enabled\n",
    "        \n",
    "        x = x.reshape(x.size(0), -1)  # Flatten the output for fully connected layer\n",
    "        x = torch.nn.functional.relu(self.fully_connected_layer(x))\n",
    "        if self.dropout_flag:\n",
    "            x = self.dropout(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(torch.nn.Module):\n",
    "    def __init__(self,device,pooling= False, dropout_flag=False):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.pooling = pooling\n",
    "        self.dropout_flag = dropout_flag\n",
    "        self.conv_layer1 =  torch.nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=2, device=device)\n",
    "        self.pool_layer1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv_layer2 = torch.nn.Conv2d(in_channels=16,out_channels=16,kernel_size=5,stride=2, device=device)\n",
    "        self.pool_layer2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        if pooling:\n",
    "            self.fully_connected_layer = nn.Linear(64,64, device=device)\n",
    "            self.final_layer = nn.Linear(64,11, device=device)\n",
    "        else:\n",
    "            self.fully_connected_layer = nn.Linear(1600, 64, device=device)\n",
    "            self.final_layer = nn.Linear(64, 11, device=device)\n",
    "        if dropout_flag:\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "    def forward(self,inp):\n",
    "        x = torch.nn.functional.relu(self.conv_layer1(inp))\n",
    "        if self.pooling:\n",
    "            x = self.pool_layer1(x)\n",
    "        x = torch.nn.functional.relu(self.conv_layer2(x))\n",
    "        if self.pooling:\n",
    "            x = self.pool_layer2(x)\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        x = torch.nn.functional.relu(self.fully_connected_layer(x))\n",
    "        if self.dropout_flag:\n",
    "            x = self.dropout(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderClass(Dataset):\n",
    "    def __init__(self,data,labels,phase,transforms):\n",
    "        super(LoaderClass, self).__init__()\n",
    "        self.transforms = transforms\n",
    "        self.labels = labels[phase + \"_labels\"]\n",
    "        self.data = data[phase + \"_data\"]\n",
    "        self.phase = phase\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = self.data[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transforms(img)\n",
    "        return img,torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,model,criterion,tr_loader,val_loader,optimizer,\n",
    "                 num_epoch,patience,batch_size,lr_scheduler=None):\n",
    "        self.model = model\n",
    "        self.tr_loader = tr_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.num_epoch = num_epoch\n",
    "        self.patience = patience\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.criterion = criterion\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.no_inc = 0\n",
    "        self.best_loss = 9999\n",
    "        self.phases = [\"train\",\"val\"]\n",
    "        self.best_model = []\n",
    "        self.best_val_acc = 0\n",
    "        self.best_train_acc = 0\n",
    "        self.best_val_loss = 0\n",
    "        self.best_train_loss = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        pass\n",
    "    def train(self):\n",
    "        pbar = tqdm.tqdm(desc= \"Epoch 0, phase: Train\",postfix=\"train_loss : ?, train_acc: ?\")\n",
    "        for i in range(self.num_epoch):\n",
    "            last_train_acc = 0\n",
    "            last_val_acc = 0\n",
    "            last_val_loss = 0\n",
    "            last_train_loss = 0\n",
    "            pbar.update(1)\n",
    "\n",
    "            for phase in self.phases:\n",
    "                total_acc = 0\n",
    "                total_loss = 0\n",
    "                start = time.time()\n",
    "                if phase == \"train\":\n",
    "                    pbar.set_description_str(\"Epoch %d,\"% i + \"phase: Training\")\n",
    "                    loader = self.tr_loader\n",
    "                    self.model.train()\n",
    "                else:\n",
    "                    pbar.set_description_str(\"Epoch %d,\"% i + \"phase: Validation\")\n",
    "                    loader = self.val_loader\n",
    "                    self.model.eval()\n",
    "                iter = 0\n",
    "                for images,labels in loader:\n",
    "                    iter += 1\n",
    "                    images = images.to(self.model.device)\n",
    "                    labels = labels.to(self.model.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    logits = self.model(images)\n",
    "                    softmaxed_scores = self.softmax(logits)\n",
    "                    _, predictions = torch.max(softmaxed_scores,1)\n",
    "                    _, labels = torch.max(labels,1)\n",
    "                    loss = self.criterion(softmaxed_scores.float(),labels.long())\n",
    "                    total_loss += loss.item()\n",
    "                    total_acc += torch.sum(predictions == labels).item()\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        pbar.set_postfix_str(\"train acc: %6.3f,\" %(total_acc/ (iter*self.batch_size)) + (\"train loss: %6.3f\" % (total_loss / iter)))\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "                    else:\n",
    "                        pass\n",
    "                        pbar.set_postfix_str(\"val acc: %6.3f,\" %(total_acc/ (iter*self.batch_size)) + (\"val loss: %6.3f\" % (total_loss / iter)))\n",
    "\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    if self.lr_scheduler:\n",
    "\n",
    "                        self.lr_scheduler.step()\n",
    "                end = time.time()\n",
    "                if phase == \"train\":\n",
    "                    loss_p = total_loss / iter\n",
    "                    acc_p = total_acc / len(self.tr_loader.dataset)\n",
    "                    last_train_acc = acc_p\n",
    "                    last_train_loss = loss_p\n",
    "                else:\n",
    "                    loss_p = total_loss / iter\n",
    "                    acc_p = total_acc / len(self.val_loader.dataset)\n",
    "                    last_val_acc = acc_p\n",
    "                    last_val_loss = loss_p\n",
    "\n",
    "                    if loss_p < self.best_loss:\n",
    "                        #print(\"New best loss, loss is: \",str(loss_p), \"acc is: \",acc_p )\n",
    "                        self.best_loss = loss_p\n",
    "                        self.no_inc = 0\n",
    "                        self.best_model = self.model\n",
    "                        self.best_train_acc = last_train_acc\n",
    "                        self.best_train_loss = last_train_loss\n",
    "                        self.best_val_loss = last_val_loss\n",
    "                        self.best_val_acc = last_val_acc\n",
    "                    else:\n",
    "                        #print(\"Not a better score\")\n",
    "\n",
    "\n",
    "                        self.no_inc += 1\n",
    "                        if self.no_inc == self.patience:\n",
    "                            print(\"Out of patience returning the best model\")\n",
    "                            print(\n",
    "                                \"Best val acc: {}, Best val loss: {}, Best train acc: {}, Best train loss: {} \".format(\n",
    "                                    self.best_val_acc, self.best_val_loss, self.best_train_acc, self.best_train_loss\n",
    "                                ))  # Stats of the best model\n",
    "                            return self.best_model\n",
    "        print(\"Training ended returning the best model\")\n",
    "        print(\n",
    "            \"Best val acc: {}, Best val loss: {}, Best train acc: {}, Best train loss: {} \".format(\n",
    "                self.best_val_acc, self.best_val_loss, self.best_train_acc, self.best_train_loss\n",
    "            ))  # Stats of the best model\n",
    "        return self.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "Momentum = 0.9 # If you use SGD with momentum\n",
    "BATCH_SIZE = 16\n",
    "POOLING = False\n",
    "NUM_EPOCHS = 200\n",
    "PATIENCE = -1\n",
    "TRAIN_PERCENT = 0.8\n",
    "VAL_PERCENT = 0.2\n",
    "NUM_ARTISTS = 11\n",
    "DATA_PATH = \"./art_data/artists\"\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Do not change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artist_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    artists = [x for x in os.listdir(DATA_PATH) if x != '.DS_Store']\n",
    "    print(artists)\n",
    "    for folder in os.listdir(DATA_PATH):\n",
    "        class_index = artists.index(folder)\n",
    "        for image_name in os.listdir(DATA_PATH + \"/\" + folder):\n",
    "            img = Image.open(DATA_PATH + \"/\" + folder + \"/\" + image_name)\n",
    "            artist_label = (np.arange(NUM_ARTISTS) == class_index).astype(np.float32)\n",
    "            data.append(np.array(img))\n",
    "            labels.append(artist_label)\n",
    "    shuffler = np.random.permutation(len(labels))\n",
    "    data = np.array(data)[shuffler]\n",
    "    labels = np.array(labels)[shuffler]\n",
    "\n",
    "    length = len(data)\n",
    "    val_size = int(length*0.2)\n",
    "    val_data = data[0:val_size+1]\n",
    "    train_data = data[val_size+1::]\n",
    "    val_labels = labels[0:val_size+1]\n",
    "    train_labels = labels[val_size+1::]\n",
    "    print(val_labels)\n",
    "    data_dict = {\"train_data\":train_data,\"val_data\":val_data}\n",
    "    label_dict = {\"train_labels\":np.array(train_labels),\"val_labels\":np.array(val_labels)}\n",
    "\n",
    "    return data_dict,label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['canaletto', 'claude monet', 'george romney', 'j. m. w. turner', 'john robert cozens', 'paul cezanne', 'paul gauguin', 'paul sandby', 'peter paul rubens', 'rembrandt', 'richard wilson']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "data,labels = load_artist_data()\n",
    "model = SimpleCNN(device=device,pooling=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LoaderClass(data,labels,\"train\",transform[\"train\"])\n",
    "valid_dataset = LoaderClass(data,labels,\"val\",transform[\"val\"])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0,phase: Training: 1it [00:00, 1198.37it/s, train_loss : ?, train_acc: ?]C:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Epoch 199,phase: Validation: 200it [04:27,  1.34s/it, val acc:  0.406,val loss:  2.110]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended returning the best model\n",
      "Best val acc: 0.4835164835164835, Best val loss: 2.047486503918966, Best train acc: 0.6722222222222223, Best train loss: 1.8831380450207253 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# standard adam optimizer, no pooling, no weight decay, no dropout, no scheduler\n",
    "trainer_m = Trainer(model, criterion, train_loader, val_loader, optimizer, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model = trainer_m.train()\n",
    "# Best val acc: 0.4835164835164835, Best val loss: 2.047486503918966, Best train acc: 0.6722222222222223, Best train loss: 1.8831380450207253 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "What is the training accuracy for your network after training? What is the validation accuracy? What do these two numbers tell you about what your network is doing?\n",
    "The training accuracy is 0.67 and the validation accuracy is 0.48. Here we can see that the network is overfitting to training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: Add Pooling Layers\n",
    "We will now add max pooling layers after each of our convolutional layers. This code has already been provided for you; all you need to do is switch the pooling flag in the hyper-parameters to True,\n",
    "and choose different values for the pooling filter size and stride. After you applied max pooling, what happened to your results? How did the training accuracy vs. validation accuracy change? What does\n",
    "that tell you about the effect of max pooling on your network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model because the other model's params got updated.\n",
    "model_pooling = SimpleCNN(device=device,pooling=True)\n",
    "opt_pooling = optim.Adam(model_pooling.parameters(), lr=LR)\n",
    "trainer_m_pooling = Trainer(model_pooling, criterion, train_loader, val_loader, opt_pooling, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_pooling = trainer_m_pooling.train()\n",
    "# Best val acc: 0.46153846153846156, Best val loss: 2.062834401925405, Best train acc: 0.675, Best train loss: 1.9000464470490166 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**<br>\n",
    "Applying pooling reduces the spatial dimensions, helping the model to focus on the most important features. This also helps with avoiding overfitting as the number of parameters are decreased.<br><br>\n",
    "The model with max pooling has decreased validation accuracy, struggles to generalize to unseen data. This is unexpected and might be because maxpooling discarded important information while downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4: Regularize Your Network!\n",
    "Because this is such a small dataset, your network is likely to overfit the data. Implement the following ways of regularizing your network. Test each one individually, and discuss how it affects your results.\n",
    "\n",
    "- __Dropout__: In PyTorch, this is implemented using the `torch.nn.dropout` class, which takes a value called the `keep_prob`, representing the probability that an activation will be dropped out. This value should be between 0.1 and 0.5 during training, and 0 for evaluation and testing. An example of how this works is available here. You should add this to your network and try different values to find one that works well.\n",
    "\n",
    "- __Weight Regularization__: You should try different optimizers, and different weight decay values for optimizers.\n",
    "\n",
    "- __Early Stopping__: Stop training your model after your validation accuracy starts to plateau or decrease (so you do not overtrain your model). The number of steps can be controlled through the `patience` hyperparameter in the code.\n",
    "\n",
    "- __Learning Rate Scheduling__: Learning rate scheduling is an important part of training neural networks. There are a lot of techniques for learning rate scheduling. You should try\n",
    "different schedulers such as `StepLR`, `CosineAnnealing`, etc.\n",
    "\n",
    "Give your results for each of these regularization techniques, and discuss which ones were the most effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**<br>\n",
    "Testing each individually:<br><br>\n",
    "Dropout (p=0.5) between the two fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = SimpleCNN(device=device,pooling=True, dropout=True)\n",
    "opt_dropout = optim.Adam(model_dropout.parameters(), lr=LR)\n",
    "trainer_m_dropout = Trainer(model_dropout, criterion, train_loader, val_loader, opt_dropout, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_dropout = trainer_m_dropout.train()\n",
    "# Best val acc: 0.4945054945054945, Best val loss: 2.027242044607798, Best train acc: 0.6333333333333333, Best train loss: 1.9216425211533257 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the training accuracy is similar to no dropout, the validation accuracy increased significantly meaning that the model generalizes well to new data.<br>\n",
    "<br>\n",
    "Weight regularization:<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = SimpleCNN(device=device,pooling=True)\n",
    "opt_sgd = torch.optim.SGD(model_sgd.parameters(), lr=LR, weight_decay=0.001, momentum=0.9)\n",
    "trainer_m_sgd = Trainer(model_sgd, criterion, train_loader, val_loader, opt_sgd, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_sgd= trainer_m_sgd.train()\n",
    "# Best val acc: 0.2967032967032967, Best val loss: 2.3858046531677246, Best train acc: 0.2722222222222222, Best train loss: 2.3842520713806152 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam1 = SimpleCNN(device=device,pooling=True)\n",
    "opt_adam1 = torch.optim.Adam(model_adam1.parameters(), lr=LR, weight_decay=0.001)\n",
    "trainer_m_adam1 = Trainer(model_adam1, criterion, train_loader, val_loader, opt_adam1, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_adam1= trainer_m_adam1.train()\n",
    "# Best val acc: 0.5054945054945055, Best val loss: 2.024585505326589, Best train acc: 0.7083333333333334, Best train loss: 1.8514255337093188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam2 = SimpleCNN(device=device,pooling=True)\n",
    "opt_adam2 = torch.optim.Adam(model_adam2.parameters(), lr=LR, weight_decay=0.01)\n",
    "trainer_m_adam2 = Trainer(model_adam2, criterion, train_loader, val_loader, opt_adam2, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_adam2= trainer_m_adam2.train()\n",
    "# Best val acc: 0.5164835164835165, Best val loss: 2.019010921319326, Best train acc: 0.7, Best train loss: 1.8626101846280305 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam3 = SimpleCNN(device=device,pooling=True)\n",
    "opt_adam3 = torch.optim.Adam(model_adam3.parameters(), lr=LR, weight_decay=0.0001)\n",
    "trainer_m_adam3 = Trainer(model_adam3, criterion, train_loader, val_loader, opt_adam3, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_adam3= trainer_m_adam3.train()\n",
    "# Best val acc: 0.46153846153846156, Best val loss: 2.057038187980652, Best train acc: 0.6861111111111111, Best train loss: 1.8569457012674082 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight regularization penalizes large weights to avoid strong dependancy on certain features thus prevent overfitting.<br>\n",
    "With SGD as optimizer and using momentum, validation accuracy decreases significantly, but this simply shows how bad SGD is. \n",
    "<br>\n",
    "\n",
    "With Adam as optimizer and weight_decay=0.001, validation accuracy increases to 0.505. I believe that this improvement is mainly caused by the adaptive learning rate of Adam, yielding a better convergence.<br>\n",
    "<br>\n",
    "With Adam as optimizer and weight_decay=0.01, validation accuracy increases significantly, meaning in our case more aggressive regularization of large weights is required. When weight_decay parameter is decreased to 0.0001, we observe worse generalization.<br>\n",
    "<br>\n",
    "Early Stopping:<br>\n",
    "(without weight decay and dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# es -> early stopping\n",
    "model_es = SimpleCNN(device=device,pooling=True)\n",
    "opt_es = torch.optim.Adam(model_es.parameters(), lr=LR)\n",
    "trainer_m_es = Trainer(model_es, criterion, train_loader, val_loader, opt_es, num_epoch=NUM_EPOCHS, patience=7,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_es= trainer_m_es.train()\n",
    "# Best val acc: 0.37362637362637363, Best val loss: 2.1712493896484375, Best train acc: 0.3638888888888889, Best train loss: 2.1862813234329224 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping is not effective at all, but I might also be unsuccesfull with setting a meaningful patience.<br>\n",
    "<br>\n",
    "Learning Rate Scheduling:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr1 = SimpleCNN(device=device,pooling=True)\n",
    "opt_lr1 = torch.optim.Adam(model_lr1.parameters(), lr=LR)\n",
    "scheduler1 = torch.optim.lr_scheduler.StepLR(opt_lr1, step_size=5, gamma=0.1)\n",
    "trainer_m_lr1 = Trainer(model_lr1, criterion, train_loader, val_loader, opt_lr1, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= scheduler1)\n",
    "best_model_lr1= trainer_m_lr1.train()\n",
    "# Best val acc: 0.2967032967032967, Best val loss: 2.2756608724594116, Best train acc: 0.2638888888888889, Best train loss: 2.305739112522291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr2 = SimpleCNN(device=device,pooling=True)\n",
    "opt_lr2 = torch.optim.Adam(model_lr2.parameters(), lr=LR)\n",
    "scheduler2 =  torch.optim.lr_scheduler.ExponentialLR(opt_lr2, gamma=0.95)\n",
    "trainer_m_lr2 = Trainer(model_lr2, criterion, train_loader, val_loader, opt_lr2, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= scheduler2)\n",
    "best_model_lr2= trainer_m_lr2.train()\n",
    "# Best val acc: 0.3516483516483517, Best val loss: 2.161716083685557, Best train acc: 0.46111111111111114, Best train loss: 2.089998447376749 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.5: Experiment with Your Architecture\n",
    "\n",
    "All those parameters at the top of `SimpleCNN` still need to be set. You cannot possibly explore all combinations; so try to change some of them individually to get some feeling for their effect (if any).\n",
    "Optionally, you can explore adding more layers. Report which changes led to the biggest increases and decreases in performance. In particular, what is the effect of making the convolutional layers have (a) a larger filter size, (b) a larger stride and (c) greater depth? How does a pyramidal-shaped network in which the feature maps gradually decrease in height and width but increase in depth compare to a flat architecture, or one with the opposite shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the same architecture using the new network class declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_flexible = FlexibleCNN(device=device, num_conv_layers=2, in_channels=3,\n",
    "                              conv_out_channels=[16,16], kernel_sizes=[5,5], strides=[2,2], pooling=True, dropout_flag=False)\n",
    "opt_flexible = optim.Adam(model_flexible.parameters(), lr=LR)\n",
    "trainer_m_flexible = Trainer(model_flexible, criterion, train_loader, val_loader, opt_flexible, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_flexible = trainer_m_flexible.train()\n",
    "# Best val acc: 0.4945054945054945, Best val loss: 2.011147916316986, Best train acc: 0.7, Best train loss: 1.8516293297643247 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, i will first examine the effect of depth on the accuracy and training time.<br>\n",
    "Using a single layer of 5x5 filters with stride=2 (thus only changing the depth)<br>\n",
    "The input to the fully connected layer is 11 x 11 x 16<br>\n",
    "This architecture yields a higher validation accuracy than using 2 5x5 layers, suggesting that 2 layers overfit to the training data.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_flexible2 = FlexibleCNN(device=device, num_conv_layers=1, in_channels=3,\n",
    "                              conv_out_channels=[16], kernel_sizes=[5], strides=[2], pooling=True, dropout_flag=True)\n",
    "opt_flexible2 = optim.Adam(model_flexible2.parameters(), lr=LR)\n",
    "trainer_m_flexible2 = Trainer(model_flexible2, criterion, train_loader, val_loader, opt_flexible2, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_flexible2 = trainer_m_flexible2.train()\n",
    "# Best val acc: 0.5604395604395604, Best val loss: 1.9935139616330464, Best train acc: 0.7305555555555555, Best train loss: 1.8350506398988806\n",
    "# Best val acc: 0.5824175824175825, Best val loss: 1.9595629771550496, Best train acc: 0.7861111111111111, Best train loss: 1.7817718360735022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using three layers of 5x5 filters with stride=2 (thus only changing the depth)<br>\n",
    "The input to the fully connected layer is 3 x 3 x 16, which is very small even though it is without pooling. This is an effect of striding with 3 conv layers.<br>\n",
    "With this architecture, I obtain worse val accuracy, the network is overfitting as it has too many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_flexible3 = FlexibleCNN(device=device, num_conv_layers=3, in_channels=3,\n",
    "                              conv_out_channels=[16,16,16], kernel_sizes=[5,5,5], strides=[2,2,2], pooling=False, dropout_flag=True)\n",
    "opt_flexible3 = optim.Adam(model_flexible3.parameters(), lr=LR)\n",
    "trainer_m_flexible3 = Trainer(model_flexible3, criterion, train_loader, val_loader, opt_flexible3, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_flexible3 = trainer_m_flexible3.train()\n",
    "# Best val acc: 0.3956043956043956, Best val loss: 2.115185578664144, Best train acc: 0.6277777777777778, Best train loss: 1.9305321081824924 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using stride=1 instead of stride=2 will shrink the receptive field of every neuron, but the network might be able to capture high resolution features more efficiently.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_flexible4 = FlexibleCNN(device=device, num_conv_layers=2, in_channels=3,\n",
    "                              conv_out_channels=[16,16], kernel_sizes=[5,5], strides=[1,1], pooling=True, dropout_flag=True)\n",
    "opt_flexible4 = optim.Adam(model_flexible4.parameters(), lr=LR)\n",
    "trainer_m_flexible4 = Trainer(model_flexible4, criterion, train_loader, val_loader, opt_flexible4, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_flexible4 = trainer_m_flexible4.train()\n",
    "# Best val acc: 0.5274725274725275, Best val loss: 2.00963286558787, Best train acc: 0.7222222222222222, Best train loss: 1.8219199698904287 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the filter size can be changed as well.<br>\n",
    "Using larger filters will enable us tp capture relations between pixels that are further away, but this might miss the features in the details.<br>\n",
    "With a smaller kernel, we will catch these details but this time miss on broader context and relations.<br>\n",
    "Experimenting with 3 x 3 and 7 x 7 kernels without changing any other architecture parameters, ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_flexible5 = FlexibleCNN(device=device, num_conv_layers=2, in_channels=3,\n",
    "                              conv_out_channels=[16,16], kernel_sizes=[3,3], strides=[2,2], pooling=True, dropout_flag=True)\n",
    "opt_flexible5 = optim.Adam(model_flexible5.parameters(), lr=LR)\n",
    "trainer_m_flexible5 = Trainer(model_flexible5, criterion, train_loader, val_loader, opt_flexible5, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_flexible5 = trainer_m_flexible5.train()\n",
    "# Best val acc: 0.4065934065934066, Best val loss: 2.1159814993540444, Best train acc: 0.5416666666666666, Best train loss: 2.015954660332721 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0,phase: Training: 1it [00:00, ?it/s, train_loss : ?, train_acc: ?]C:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Epoch 199,phase: Validation: 200it [04:23,  1.32s/it, val acc:  0.458,val loss:  2.043]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended returning the best model\n",
      "Best val acc: 0.5274725274725275, Best val loss: 2.0259124835332236, Best train acc: 0.6555555555555556, Best train loss: 1.8992950294328772 \n"
     ]
    }
   ],
   "source": [
    "model_flexible6 = FlexibleCNN(device=device, num_conv_layers=2, in_channels=3,\n",
    "                              conv_out_channels=[16,16], kernel_sizes=[7,7], strides=[2,2], pooling=True, dropout_flag=True)\n",
    "opt_flexible6 = optim.Adam(model_flexible6.parameters(), lr=LR)\n",
    "trainer_m_flexible6 = Trainer(model_flexible6, criterion, train_loader, val_loader, opt_flexible6, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_flexible6 = trainer_m_flexible6.train()\n",
    "# Best val acc: 0.6043956043956044, Best val loss: 1.9428099195162456, Best train acc: 0.7138888888888889, Best train loss: 1.851250321968742"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger filter sizes produce better results, meaning that in our problem, larger receptive field is crucial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.6: Optimize Your Architecture\n",
    "Based on your experience with these tests, try to achieve the best performance that you can on the validation set by varying the hyperparameters, architecture, and regularization methods. You can even (optionally) try to think of additional ways to augment the data, or experiment with techniques like local response normalization layers using `torch.nn.LocalResponseNorm` or weight normalization using the implementation [here](https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html#weight_norm). Report the best performance you are able to achieve, and the settings you used to obtain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with kernel sizes = [9, 3, 3], and strides=[2, 2, 1] and pooling=True: the input to fc is: 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_fc_input_size(pooling, kernel_sizes, strides):\n",
    "        w_in = 50\n",
    "        for f, s in zip(kernel_sizes, strides):\n",
    "            w_in = math.floor((w_in - f)/s) +1\n",
    "            if pooling:\n",
    "                # filter size and stride both equal to 2 by defaul for pooling\n",
    "                w_in = math.floor((w_in - 2) / 2) + 1\n",
    "        return w_in\n",
    "kernel_sizes=[9,3,3]\n",
    "strides=[2,2,1]\n",
    "pooling=True\n",
    "x = compute_fc_input_size(pooling=pooling, kernel_sizes=kernel_sizes, strides=strides)\n",
    "print(f'with kernel sizes = {kernel_sizes}, and strides={strides} and pooling={pooling}: the input to fc is: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0,phase: Training: 1it [00:00, ?it/s, train_loss : ?, train_acc: ?]C:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "Epoch 199,phase: Validation: 200it [05:28,  1.64s/it, val acc:  0.479,val loss:  2.062]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended returning the best model\n",
      "Best val acc: 0.5054945054945055, Best val loss: 2.026154577732086, Best train acc: 0.7472222222222222, Best train loss: 1.7959446751553079 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n9,9 - 2,2 - no pool \\nEpoch 199,phase: Validation: 200it [01:38,  2.03it/s, val acc:  0.469,val loss:  2.049]\\nTraining ended returning the best model\\nBest val acc: 0.5164835164835165, Best val loss: 2.015752991040548, Best train acc: 0.7472222222222222, Best train loss: 1.8079308478728584 \\n\\n7,3,3 - 2,2,1 - no pool - w_d = 0.0005 - 16,64,64\\nEpoch 199,phase: Validation: 200it [02:34,  1.30it/s, val acc:  0.490,val loss:  2.034]    \\nTraining ended returning the best model\\nBest val acc: 0.5494505494505495, Best val loss: 1.9998568296432495, Best train acc: 0.725, Best train loss: 1.8211293116859768 \\n\\n7,3,3 - 2,2,1 - no pool - w_d = 0.0001 - 16,64,64\\nEpoch 199,phase: Validation: 200it [02:52,  1.16it/s, val acc:  0.458,val loss:  2.065]    \\nTraining ended returning the best model\\nBest val acc: 0.5384615384615384, Best val loss: 1.9924977620442708, Best train acc: 0.7583333333333333, Best train loss: 1.788537139477937\\n\\n9,3,3 - 2,2,1 - no pool - 16,64,64 - w_d = 0.0005\\nEpoch 199,phase: Validation: 200it [06:09,  1.85s/it, val acc:  0.438,val loss:  2.084]    \\nTraining ended returning the best model\\nBest val acc: 0.5274725274725275, Best val loss: 2.0125731229782104, Best train acc: 0.7611111111111111, Best train loss: 1.7932016849517822\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LR = 0.0005\n",
    "model_optim = FlexibleCNN(device=device, num_conv_layers=3, in_channels=3,\n",
    "                              conv_out_channels=[16,64,64], kernel_sizes=[7,3,3], strides=[2,2,1], pooling=False, dropout_flag=True)\n",
    "opt_optim = optim.Adam(model_optim.parameters(), lr=LR, weight_decay=0.001)\n",
    "trainer_m_optim = Trainer(model_optim, criterion, train_loader, val_loader, opt_optim, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_optim = trainer_m_optim.train()\n",
    "\"\"\"\n",
    "9,9 - 2,2 - no pool \n",
    "Epoch 199,phase: Validation: 200it [01:38,  2.03it/s, val acc:  0.469,val loss:  2.049]\n",
    "Training ended returning the best model\n",
    "Best val acc: 0.5164835164835165, Best val loss: 2.015752991040548, Best train acc: 0.7472222222222222, Best train loss: 1.8079308478728584 \n",
    "\n",
    "7,3,3 - 2,2,1 - no pool - w_d = 0.0005 - 16,64,64\n",
    "Epoch 199,phase: Validation: 200it [02:34,  1.30it/s, val acc:  0.490,val loss:  2.034]    \n",
    "Training ended returning the best model\n",
    "Best val acc: 0.5494505494505495, Best val loss: 1.9998568296432495, Best train acc: 0.725, Best train loss: 1.8211293116859768 \n",
    "\n",
    "7,3,3 - 2,2,1 - no pool - w_d = 0.0001 - 16,64,64\n",
    "Epoch 199,phase: Validation: 200it [02:52,  1.16it/s, val acc:  0.458,val loss:  2.065]    \n",
    "Training ended returning the best model\n",
    "Best val acc: 0.5384615384615384, Best val loss: 1.9924977620442708, Best train acc: 0.7583333333333333, Best train loss: 1.788537139477937\n",
    "\n",
    "9,3,3 - 2,2,1 - no pool - 16,64,64 - w_d = 0.0005\n",
    "Epoch 199,phase: Validation: 200it [06:09,  1.85s/it, val acc:  0.438,val loss:  2.084]    \n",
    "Training ended returning the best model\n",
    "Best val acc: 0.5274725274725275, Best val loss: 2.0125731229782104, Best train acc: 0.7611111111111111, Best train loss: 1.7932016849517822\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.7: Test Your Final Architecture on Variations of the Data\n",
    "In PyTorch data augmentation can be done dynamically while loading the data using what they call `transforms`. Note that some of the transforms are already implemented. You can\n",
    "try other transformations, such as the ones shown in Figure 3 and also try different probabilities for these transformations. You may find [this link](https://pytorch.org/vision/stable/transforms.html) helpful. Note that the PyTorch data loader refreshes the\n",
    "data in each epoch and apply different transformations to the different instances.\n",
    "\n",
    "Now that you have optimized your architecture, you are ready to test it on augmented data!\n",
    "Report your performance on each of the transformed datasets. Are you surprised by any of the results?\n",
    "Which transformations is your network most invariant to, and which lead it to be unable to recognize the images? What does that tell you about what features your network has learned to use to recognize artists’ images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    }\n",
    "train_dataset = LoaderClass(data,labels,\"train\",transform[\"train\"])\n",
    "valid_dataset = LoaderClass(data,labels,\"val\",transform[\"val\"])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Transfer Learning with Deep Network\n",
    "\n",
    "In this part, you will fine-tune AlexNet model pretrained on ImageNet to recognize faces. For the sake of simplicity you may use [the pretrained AlexNet model](https://pytorch.org/hub/pytorch_vision_alexnet/) provided in PyTorch Hub. You will\n",
    "work with a subset of the FaceScrub dataset. The subset of male actors is [here](http://www.cs.toronto.edu/~guerzhoy/321/proj1/subset_actors.txt) and the subset of female actors is [here](http://www.cs.toronto.edu/~guerzhoy/321/proj1/subset_actresses.txt). The dataset consists of URLs of images with faces, as well as the bounding boxes of the faces. The format of the bounding box is as follows (from the FaceScrub `readme.txt` file):\n",
    "\n",
    "` \n",
    "The format is x1,y1,x2,y2, where (x1,y1) is the coordinate of the top-left corner of the bounding box and (x2,y2) is that of the bottom-right corner, with (0,0) as the top-left corner of the image. Assuming the image is represented as a Python NumPy array I, a face\n",
    "in I can be obtained as I[y1:y2, x1:x2].\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find it helpful to use and/or modify [this script](www.cs.toronto.edu/~guerzhoy/321/proj1/get_data.py) for downloading the image data. Note that you should crop out the images of the faces and resize them to appropriate size before proceeding further. Make sure to check the SHA-256 hashes, and make sure to only keep faces for which the hashes match. You should set aside 70 images per faces for the training set, and use the rest for the test and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: Train a Multilayer Perceptron\n",
    "First resize the images to 28 × 28 pixels. Use a fully-connected neural network with a single hidden layer of size 300 units.\n",
    "Below, include the learning curve for the test, training, and validation sets, and the final performance classification on the test set. Include a text description of your system. In particular, describe how you preprocessed the input and initialized the weights, what activation function you used, and what the exact architecture of the network that you selected was. You might get performances close to 80-85% accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\site-packages\\ipykernel_launcher.py:81: DeprecationWarning:     `imread` is deprecated!\n",
      "    `imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``imageio.imread`` instead.\n",
      "c:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\site-packages\\ipykernel_launcher.py:85: DeprecationWarning:     `imresize` is deprecated!\n",
      "    `imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "    Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noth0.jpg\n",
      "noth1.jpg\n",
      "noth2.jpg\n",
      "noth3.jpg\n",
      "noth4.jpg\n",
      "noth5.jpg\n",
      "noth6.jpg\n",
      "noth7.jpg\n",
      "noth8.jpg\n",
      "noth9.jpg\n",
      "noth10.jpg\n",
      "noth11.jpg\n",
      "noth12.jpg\n",
      "noth13.jpg\n",
      "noth14.jpg\n",
      "noth15.jpg\n",
      "noth16.jpg\n",
      "noth17.jpg\n",
      "noth18.jpg\n",
      "noth19.png\n",
      "noth20.jpg\n",
      "noth21.jpg\n",
      "noth22.jpg\n",
      "noth23.jpg\n",
      "klein0.jpg\n",
      "klein1.jpg\n",
      "klein2.jpg\n",
      "klein3.jpg\n",
      "klein4.jpg\n",
      "klein5.jpg\n",
      "klein6.jpg\n",
      "klein7.jpg\n",
      "klein8.jpg\n",
      "klein9.jpg\n",
      "klein10.jpg\n",
      "statham0.jpg\n",
      "statham1.jpg\n",
      "statham2.jpg\n",
      "statham3.jpg\n",
      "statham4.jpg\n",
      "statham5.jpg\n",
      "statham6.jpg\n",
      "statham7.jpg\n",
      "statham8.jpg\n",
      "statham9.jpg\n",
      "statham10.jpg\n",
      "statham11.jpg\n",
      "statham12.jpg\n",
      "butler0.jpg\n",
      "butler1.jpg\n",
      "butler2.jpg\n",
      "butler3.jpg\n",
      "butler4.jpg\n",
      "butler5.jpg\n",
      "butler6.jpg\n",
      "butler7.jpg\n",
      "butler8.jpg\n",
      "butler9.jpg\n",
      "butler10.jpg\n",
      "butler11.jpg\n",
      "butler12.jpg\n",
      "butler13.jpg\n",
      "butler14.jpg\n",
      "butler15.jpg\n",
      "butler16.jpg\n",
      "butler17.jpg\n",
      "butler18.jpg\n",
      "butler19.jpg\n",
      "butler20.jpg\n",
      "butler21.jpg\n",
      "butler22.jpg\n",
      "butler23.jpg\n",
      "butler24.jpg\n",
      "butler25.jpg\n",
      "butler26.jpg\n",
      "butler27.jpg\n",
      "butler28.jpg\n",
      "butler29.jpg\n",
      "butler30.jpg\n",
      "butler31.jpg\n",
      "butler32.png\n",
      "butler33.jpg\n",
      "butler34.jpg\n",
      "butler35.jpg\n",
      "richter0.jpg\n",
      "richter1.jpg\n",
      "richter2.jpg\n",
      "richter3.jpg\n",
      "richter4.jpg\n",
      "richter5.jpg\n",
      "richter6.jpg\n",
      "richter7.jpg\n",
      "richter8.jpg\n",
      "richter9.jpg\n",
      "richter10.jpg\n",
      "richter11.jpg\n",
      "richter12.png\n",
      "richter13.jpg\n",
      "richter14.jpg\n",
      "richter15.JPG\n",
      "richter16.jpg\n",
      "richter17.jpg\n",
      "richter18.jpg\n",
      "richter19.jpg\n",
      "richter20.jpg\n",
      "richter21.png\n",
      "richter22.jpg\n",
      "richter23.jpg\n",
      "richter24.png\n",
      "richter25.jpg\n",
      "richter26.jpg\n",
      "richter27.jpg\n",
      "richter28.jpg\n",
      "richter29.jpg\n",
      "richter30.jpg\n",
      "richter31.jpg\n",
      "walker0.jpg\n",
      "walker1.jpg\n",
      "walker2.jpg\n",
      "walker3.jpg\n",
      "walker4.jpg\n",
      "walker5.jpg\n",
      "walker6.jpg\n",
      "walker7.jpg\n",
      "walker8.jpg\n",
      "walker9.jpg\n",
      "walker10.jpg\n",
      "walker11.jpg\n",
      "walker12.jpg\n",
      "walker13.jpg\n",
      "walker14.jpg\n",
      "walker15.jpg\n",
      "walker16.jpg\n",
      "walker17.jpg\n",
      "walker18.jpg\n",
      "walker19.jpg\n",
      "walker20.jpg\n",
      "walker21.jpg\n",
      "walker22.jpg\n",
      "walker23.jpg\n",
      "walker24.jpg\n",
      "walker25.jpg\n",
      "walker26.jpg\n",
      "walker27.jpg\n",
      "walker28.jpg\n",
      "walker29.jpg\n",
      "walker30.jpg\n",
      "walker31.jpg\n",
      "walker32.jpg\n",
      "walker33.jpg\n",
      "walker34.jpg\n",
      "walker35.jpg\n",
      "walker36.jpg\n",
      "walker37.jpg\n",
      "walker38.jpg\n",
      "walker39.jpg\n",
      "walker40.jpg\n",
      "walker41.jpg\n",
      "walker42.jpg\n",
      "radcliffe0.jpg\n",
      "radcliffe1.jpg\n",
      "radcliffe2.jpg\n",
      "radcliffe3.jpg\n",
      "radcliffe4.jpg\n",
      "radcliffe5.jpg\n",
      "radcliffe6.jpg\n",
      "radcliffe7.jpg\n",
      "radcliffe8.jpg\n",
      "radcliffe9.jpg\n",
      "radcliffe10.jpg\n",
      "radcliffe11.jpg\n",
      "radcliffe12.jpg\n",
      "radcliffe13.jpg\n",
      "radcliffe14.jpg\n",
      "radcliffe15.jpg\n",
      "radcliffe16.jpg\n",
      "radcliffe17.jpg\n",
      "radcliffe18.jpg\n",
      "radcliffe19.jpg\n",
      "radcliffe20.jpg\n",
      "radcliffe21.png\n",
      "radcliffe22.jpg\n",
      "radcliffe23.jpg\n",
      "radcliffe24.jpg\n",
      "radcliffe25.jpg\n",
      "radcliffe26.jpg\n",
      "radcliffe27.jpg\n",
      "radcliffe28.jpg\n",
      "radcliffe29.jpg\n",
      "radcliffe30.jpg\n",
      "radcliffe31.jpg\n",
      "radcliffe32.jpg\n",
      "radcliffe33.jpg\n",
      "radcliffe34.jpg\n",
      "radcliffe35.jpg\n",
      "radcliffe36.jpg\n",
      "radcliffe37.jpg\n",
      "radcliffe38.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3584\\4290474171.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mcorrectHash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./uncropped/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./uncropped/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3584\\4290474171.py\u001b[0m in \u001b[0;36mtimeout\u001b[1;34m(func, args, kwargs, timeout_duration, default)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInterruptableThread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout_duration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misAlive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[1;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "import random\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from scipy.ndimage import filters\n",
    "import urllib\n",
    "from hashlib import sha256\n",
    "#from rgb2gray import rgb2gray\n",
    "\n",
    "\n",
    "# Instructions to run the code:\n",
    "# the two paths I used below are my local paths, \"uncropped/\" and \"cropped/\" folders\n",
    "# should be created at the same location where this python file exists. The code\n",
    "# will download the images automatically and it's implemented the way such that\n",
    "# the gray scale images are generated and cropped right after the image is\n",
    "# downloaded. \"faces.txt\" file is a txt file which contains all info from\n",
    "# \"subset_actors.txt\" and \"subset_actresses.txt\", so the code can handle all\n",
    "# required actors/actresses at one time.\n",
    "\n",
    "act = list(set([a.split(\"\\t\")[0] for a in open(\"subset_actors.txt\").readlines()]))\n",
    "\n",
    "def timeout(func, args=(), kwargs={}, timeout_duration=1, default=None):\n",
    "    '''From:\n",
    "    http://code.activestate.com/recipes/473878-timeout-function-using-threading/'''\n",
    "    import threading\n",
    "    class InterruptableThread(threading.Thread):\n",
    "        def __init__(self):\n",
    "            threading.Thread.__init__(self)\n",
    "            self.result = None\n",
    "\n",
    "        def run(self):\n",
    "            try:\n",
    "                self.result = func(*args, **kwargs)\n",
    "            except:\n",
    "                self.result = default\n",
    "\n",
    "    it = InterruptableThread()\n",
    "    it.start()\n",
    "    it.join(timeout_duration)\n",
    "    if it.isAlive():\n",
    "        return False\n",
    "    else:\n",
    "        return it.result\n",
    "\n",
    "testfile = urllib.request.URLopener()            \n",
    "\n",
    "path1 = './uncropped/'\n",
    "path2 = './cropped/'\n",
    "# First loop through the follwing actors' images in uncropped folder\n",
    "#act = ['Gerard Butler', 'Daniel Radcliffe', 'Michael Vartan', 'Lorraine Bracco', 'Peri Gilpin', 'Angie Harmon'] \n",
    "gray()\n",
    "for a in act:\n",
    "    name = a.split()[1].lower()\n",
    "    i = 0\n",
    "    # This faces.txt contains all actors and actresses\n",
    "    for line in open(\"subset_actors.txt\"):\n",
    "        if a in line:\n",
    "            filename = name+str(i)+'.'+line.split()[4].split('.')[-1]\n",
    "            x1 = int(line.split()[5].split(',')[0])\n",
    "            y1 = int(line.split()[5].split(',')[1])  \n",
    "            x2 = int(line.split()[5].split(',')[2]) \n",
    "            y2 = int(line.split()[5].split(',')[3])\n",
    "            correctHash = line.split()[6]\n",
    "            timeout(testfile.retrieve, (line.split()[4], \"./uncropped/\"+filename), {}, 30)\n",
    "            if not os.path.isfile(\"./uncropped/\"+filename):\n",
    "                continue\n",
    "            else:\n",
    "                # Filter out the corrupted images\n",
    "                file = open(\"./uncropped/\"+filename, \"rb\").read()\n",
    "                fileHash = sha256(file).hexdigest()\n",
    "                if fileHash != correctHash:\n",
    "                    continue\n",
    "                try:\n",
    "                    # Now crop the image at each loop\n",
    "                    j = imread(\"./uncropped/\"+filename)\n",
    "                    # Crop the image at each loop and call rgb2gray function\n",
    "                    out = j[y1:y2, x1:x2]\n",
    "                    # Resize the image and save it\n",
    "                    out = imresize(out, [28, 28])\n",
    "                    imsave(\"./cropped/\"+filename, out)\n",
    "                except: # Handle the unexpected runtime errors\n",
    "                    continue\n",
    "            print(filename)b\n",
    "            \n",
    "            i += 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorFacesDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_num = 0\n",
    "        \n",
    "       \n",
    "        self.actors = sorted(os.listdir(root_dir))\n",
    "        \n",
    "        for label, actor in enumerate(self.actors):\n",
    "            self.class_num += 1\n",
    "            actor_folder = os.path.join(root_dir, actor)\n",
    "            \n",
    "            if os.path.isdir(actor_folder):\n",
    "                for image_name in os.listdir(actor_folder):\n",
    "                    if image_name.endswith(('.jpg', '.png', '.jpeg')): \n",
    "                        image_path = os.path.join(actor_folder, image_name)\n",
    "                        self.image_paths.append(image_path)\n",
    "                        self.labels.append(label)  # Use actor index as label\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),  # Resize\n",
    "    transforms.ToTensor(),  # Convert image to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize to ImageNet stats\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'actor_faces'\n",
    "\n",
    "dataset = ActorFacesDataset(root_dir=root_dir, transform=transform)\n",
    "\n",
    "train_images, testval_images = train_test_split(dataset.image_paths, test_size=0.3, random_state=42)\n",
    "val_images, test_images = train_test_split(testval_images, test_size=0.5, random_state=42)\n",
    "\n",
    "train_labels = [dataset.labels[dataset.image_paths.index(img)] for img in train_images]\n",
    "val_labels = [dataset.labels[dataset.image_paths.index(img)] for img in val_images]\n",
    "test_labels = [dataset.labels[dataset.image_paths.index(img)] for img in test_images]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(len(train_images)))\n",
    "val_dataset = torch.utils.data.Subset(dataset, range(len(train_images), len(train_images) + len(val_images)))\n",
    "test_dataset = torch.utils.data.Subset(dataset, range(len(train_images) + len(val_images), len(dataset)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for images, labels in train_loader:\n",
    "    print(images[0].shape) \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.relu = nn.ReLU()                      \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim=1)     \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], -1) # Flatten input\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "input_dim = 28 * 28 *3 \n",
    "hidden_dim = 300    \n",
    "output_dim = dataset.class_num\n",
    "print(output_dim)\n",
    "\n",
    "model = FCNN(input_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            #print(f'outputs: {outputs.shape}')\n",
    "            #print(f'labels: {labels.shape}')\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation Loss\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "\n",
    "                #print(f'outputs: {outputs.shape}')\n",
    "                #print(f'labels: {labels.shape}')\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "epochs = 5\n",
    "train_losses, val_losses = train(model, train_loader, val_loader, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223,\n",
      "        223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223,\n",
      "        223, 223, 223, 223])\n",
      "predictions: tensor([ 39, 127, 134, 134,  89,   1, 126, 157, 127, 157, 153,   4,  62,  88,\n",
      "         49,   1, 144,  39,  77, 125,  62,  39,  39,  81,  21,  53,  61,  39,\n",
      "         62, 127,  56,  77])\n",
      "labels: tensor([223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223,\n",
      "        223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223,\n",
      "        223, 223, 223, 223])\n",
      "predictions: tensor([ 39,  93, 173,  75,  39, 153,  30,  26,  39,  77, 127,  71,   1, 126,\n",
      "        119,   1, 147, 157, 151,  53,  90, 127, 157,  39,   1,  23, 157,  48,\n",
      "         39, 125,  37, 134])\n",
      "labels: tensor([223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223,\n",
      "        223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223, 223,\n",
      "        223, 223, 223, 223])\n",
      "predictions: tensor([ 56, 157,  62,  39,   4,  75,  39,  39,  30,  75,  81, 128,  61, 164,\n",
      "        128,  77,  48, 157,   1, 127,  75,  39,  62, 157,   5,  81,  75, 125,\n",
      "        157,  15, 151,  26])\n",
      "labels: tensor([223, 223, 223, 223, 223, 223, 223, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224])\n",
      "predictions: tensor([127,  39, 146,  60, 128, 153, 134, 164,  12, 127, 152,  49, 146,  39,\n",
      "         67,  89,  12,  89, 154, 108,   9, 109, 149,  92, 132,  89,  63, 153,\n",
      "        146, 131, 121,  39])\n",
      "labels: tensor([224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224])\n",
      "predictions: tensor([ 39,  30,  30,  89,  96,  89, 131,  30, 132, 132,  39,  59,  76, 157,\n",
      "         21, 164, 132,  30, 119, 150, 147,  30, 131, 146,  39, 131,  15, 131,\n",
      "         21, 164,  57,  99])\n",
      "labels: tensor([224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224,\n",
      "        224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 225, 225,\n",
      "        225, 225, 225, 225])\n",
      "predictions: tensor([131, 131, 131, 143, 149,  21, 164,  89, 132,  21, 157, 164,  69,  72,\n",
      "        131,  15, 131,  72,  57,  39,  21, 131,  89,  89, 131, 132,  50,  68,\n",
      "         72, 128,  72,  63])\n",
      "labels: tensor([225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225,\n",
      "        225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225,\n",
      "        225, 225, 225, 225])\n",
      "predictions: tensor([167,  72,  72,  89, 138,  72,  64,  30,  38, 128,  72,  59, 149,  72,\n",
      "         72,  91, 149, 101,   2, 160, 167, 167,  43,  14,  14, 161,  72,  37,\n",
      "         50, 149,  63, 134])\n",
      "labels: tensor([225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225,\n",
      "        225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225, 225,\n",
      "        225, 225, 225, 226])\n",
      "predictions: tensor([ 72,  41,  72, 167, 167, 111,  41,  41, 167,  89, 164,  72, 149, 131,\n",
      "         91, 149, 173, 167,  79, 167, 126, 127,  12,  63, 132, 127,  39, 172,\n",
      "         50,  77,  45,  63])\n",
      "labels: tensor([226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226,\n",
      "        226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226,\n",
      "        226, 226, 226, 226])\n",
      "predictions: tensor([  7, 173,  57, 109,  42,  23, 130, 109,  41,   7, 167,  92, 180, 109,\n",
      "        136,  69,  42,  14, 178,  50,  57, 109,  64,  97, 136, 128,  60,  11,\n",
      "        109,  41, 109, 109])\n",
      "labels: tensor([226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226,\n",
      "        226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226,\n",
      "        226, 226, 226, 226])\n",
      "predictions: tensor([114,  92,  17, 109,   7,  17, 109,  74, 110,  92,  92,   7, 173, 173,\n",
      "        173,  41,  17,  23,  42, 114,   7,  42,  41,  64,   9,  86,  67,  14,\n",
      "         42,  42, 117,  67])\n",
      "labels: tensor([226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226,\n",
      "        226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 226, 227, 227,\n",
      "        227, 227, 227, 227])\n",
      "predictions: tensor([ 92,  42,  32,  17, 108,  42, 109,  45, 160, 114,   9,   9,  92,  17,\n",
      "        118,  67,  74, 108,   9,  95,  50,  41,  31,  96,  57, 110,  36, 100,\n",
      "         62,  62, 170,  39])\n",
      "labels: tensor([227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227,\n",
      "        227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227,\n",
      "        227, 227, 227, 227])\n",
      "predictions: tensor([177, 127,  95,  62, 109, 126, 100,  95,  92, 122,  25, 167, 108,  62,\n",
      "        164, 127,  39,  22,  15, 137,  15,  39,  39, 126, 130,  33,  82,  39,\n",
      "        173,  15,  52, 109])\n",
      "labels: tensor([227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227,\n",
      "        227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227, 227,\n",
      "        227, 227, 227, 227])\n",
      "predictions: tensor([ 39, 170,  39,  31,  92,  13,  96, 102, 119,  92, 127, 108, 126, 173,\n",
      "         62, 122,  39, 180,  96,  66,  39,  39,  48, 170, 100,  62, 167,  81,\n",
      "        127,  39, 138,  89])\n",
      "labels: tensor([227, 227, 227, 227, 227, 227, 227, 228, 228, 228, 228, 228, 228, 228,\n",
      "        228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228,\n",
      "        228, 228, 228, 228])\n",
      "predictions: tensor([177, 173,  62,  15, 109, 173,  39, 167,  92,  15,  15,  15, 137, 148,\n",
      "        143,  70,  90, 115, 148,  39,  39, 153, 167,  14, 135, 153,  15,  70,\n",
      "         96, 128,  15,  43])\n",
      "labels: tensor([228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228,\n",
      "        228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228,\n",
      "        228, 228, 228, 228])\n",
      "predictions: tensor([ 10,  15,  70, 167, 176,  15,  70,   9,  39,   4, 135,  67,  39,  92,\n",
      "        135,  76,  96,  39,  15,  15,  15, 135, 167,  70, 167,  39, 164, 149,\n",
      "         15, 148,  71, 176])\n",
      "labels: tensor([228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228,\n",
      "        228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228,\n",
      "        228, 228, 228, 228])\n",
      "predictions: tensor([136,  90, 177,  96, 148, 110, 167, 153,  70, 167, 177,  15, 176, 148,\n",
      "         64,  83,  56,  70,  91,  22,  70,  70,  43, 136,  70, 153,  15, 129,\n",
      "         39,  39,  70,  92])\n",
      "labels: tensor([228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228, 228,\n",
      "        228, 228, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229,\n",
      "        229, 229, 229, 229])\n",
      "predictions: tensor([135,  15,  73,  92,  99, 128,  39,  96,  39,  64,  39,  15,  39,  92,\n",
      "         98,  72,  90, 117,  68, 154,  73, 136, 118,  68, 118, 154, 155,  54,\n",
      "        104,  90, 154,  73])\n",
      "labels: tensor([229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229,\n",
      "        229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229,\n",
      "        229, 229, 229, 229])\n",
      "predictions: tensor([143, 155, 154,   2, 155,  14,  50, 155, 118, 143,  73, 164,  82, 113,\n",
      "         58,  12,  73,  15, 154,   6,  72,  17,  65,   6, 154,  49,  14, 104,\n",
      "         57, 173,  58,  73])\n",
      "labels: tensor([229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 229, 230, 230,\n",
      "        230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230,\n",
      "        230, 230, 230, 230])\n",
      "predictions: tensor([ 65,  20, 104, 128,  50,  73,   7, 104, 155, 155,  95,  58,  91,  72,\n",
      "        160,  91,  18,  30, 100, 132,  21, 125,  41,  89,  42,  99, 132,   9,\n",
      "        132,  42, 128, 176])\n",
      "labels: tensor([230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230,\n",
      "        230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230,\n",
      "        230, 230, 230, 230])\n",
      "predictions: tensor([176, 132,  62,  89, 177,  21, 131, 149,  39, 132, 149,  89,  72, 132,\n",
      "         30,  72,  41,  30,  26,  38,  49, 149,  91,  91, 127, 132,   1,  72,\n",
      "         38, 149,  89,  37])\n",
      "labels: tensor([230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230,\n",
      "        230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230,\n",
      "        230, 230, 230, 230])\n",
      "predictions: tensor([167,  63, 131, 115, 102, 172, 149,  48,  37,  76, 147, 131,  38,  42,\n",
      "         30,  72, 132,  66, 177,  91,  68, 149,  38, 149,  21,  21, 149,  72,\n",
      "         39, 132, 167,  50])\n",
      "labels: tensor([230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230, 230,\n",
      "        230, 230, 230, 230, 230, 230, 231, 231, 231, 231, 231, 231, 231, 231,\n",
      "        231, 231, 231, 231])\n",
      "predictions: tensor([180, 139,  30, 149,  21, 127,  72,  91, 139, 160,  89, 176,  43,  30,\n",
      "         72,  12,  21,  38,  91,  38, 167,  97, 154, 127,  95, 116,  62,  66,\n",
      "        170,  39,  39,  39])\n",
      "labels: tensor([231, 231, 231, 231, 231, 231, 231, 231, 231, 231, 231, 231, 231, 231,\n",
      "        231, 231, 231, 231, 231, 231, 231, 231, 231, 231, 232, 232, 232, 232,\n",
      "        232, 232, 232, 232])\n",
      "predictions: tensor([110, 149,  65, 137, 136,  68, 137,  26,  12,  15,  97,  82,  96, 149,\n",
      "          6,  97, 167, 149,  35,  96, 149,  99,  57, 127,  52, 145,  96, 176,\n",
      "         42,  75,  66, 129])\n",
      "labels: tensor([232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232,\n",
      "        232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232,\n",
      "        232, 232, 232, 232])\n",
      "predictions: tensor([129, 147,  99,  96, 107, 111,  96, 102,  42,   9,  66,  96,  17, 176,\n",
      "          0,  99, 129,  18, 176,  99,  14, 136, 108, 129, 124,  66,  99, 124,\n",
      "         51,  96,  14, 139])\n",
      "labels: tensor([232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232, 232,\n",
      "        232, 232, 232, 232, 232, 232, 233, 233, 233, 233, 233, 233, 233, 233,\n",
      "        233, 233, 233, 233])\n",
      "predictions: tensor([ 92, 110, 139,  96, 129,  97,  99,  17,   9, 178, 139, 129,   4, 102,\n",
      "        176, 171, 128, 132,  96,  64,   0, 167,   7,   3,  98,  57, 163,  91,\n",
      "        115, 115,  93,   4])\n",
      "labels: tensor([233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233,\n",
      "        233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233,\n",
      "        233, 233, 233, 233])\n",
      "predictions: tensor([ 71, 109, 128,  45,  76,  89,  98, 114, 176,  42, 110, 163,  23, 167,\n",
      "        167,  23,  42,  57, 159, 115, 155, 115,   4,  10, 119,   8, 126,  42,\n",
      "         21,  68,  74,  75])\n",
      "labels: tensor([233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233,\n",
      "        233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233, 233,\n",
      "        233, 233, 233, 233])\n",
      "predictions: tensor([ 43,  13,  57,  57,  42,  99,   7,  14, 115,  42,  14,  61,   9, 176,\n",
      "        148, 114,  23,  57,  45,  42,  15,  76, 163,   6, 167, 167, 176, 115,\n",
      "          0, 163, 148,   8])\n",
      "labels: tensor([233, 233, 233, 233, 233, 233, 234, 234, 234, 234, 234, 234, 234, 234,\n",
      "        234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234,\n",
      "        234, 234, 234, 234])\n",
      "predictions: tensor([ 41,   4,   8,  42,   6,  42,  46, 164, 173, 108, 173, 109, 173,   1,\n",
      "         15,  70, 147, 169, 164,  30,  89, 164, 109,  56,  56, 132, 164, 125,\n",
      "        164,  15, 149,  56])\n",
      "labels: tensor([234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234,\n",
      "        234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234,\n",
      "        234, 234, 234, 234])\n",
      "predictions: tensor([164,  46, 138,  46, 173, 154,  62, 132, 173,  89, 108,  91, 175, 161,\n",
      "         49, 132,  15,  92,  72, 154,   9, 164, 152, 164, 128,  68,  46, 164,\n",
      "        126, 175, 138,  91])\n",
      "labels: tensor([234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234,\n",
      "        234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 234, 235, 235, 235,\n",
      "        235, 235, 235, 235])\n",
      "predictions: tensor([176, 177,  15, 173,  68,  70,  91, 173,  17, 122, 164,  41,  38, 176,\n",
      "        173,  30, 177,  15,  35,  34, 161, 132, 138, 132,  68,  30,   4,  26,\n",
      "         22,  68,  22,  45])\n",
      "labels: tensor([235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235,\n",
      "        235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235,\n",
      "        235, 235, 235, 235])\n",
      "predictions: tensor([136, 138,  23,   4, 143, 140,  53,  22, 176, 110, 143,  68,  23, 140,\n",
      "        135, 119, 135,  22,  43,  97, 128, 153, 143,   0, 158,  22,  13,  23,\n",
      "         15, 128,  70,  71])\n",
      "labels: tensor([235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235,\n",
      "        235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235, 235,\n",
      "        235, 235, 236, 236])\n",
      "predictions: tensor([151,  50,   4, 110, 135,  22,  22, 151,  56, 119, 144,  22, 135, 167,\n",
      "        138,  68,  71, 113, 143,  68, 110,  55,  26, 135,  50,  22,  22, 140,\n",
      "        135, 138,  77, 149])\n",
      "labels: tensor([236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236,\n",
      "        236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236,\n",
      "        236, 236, 236, 236])\n",
      "predictions: tensor([ 79, 150, 129, 149,  96,  97,  17, 146, 150, 149, 115,  49,  39,  10,\n",
      "        129, 129, 115,  96, 176, 129,  75,  42, 119, 149, 119,  30,  62,  96,\n",
      "         67, 111,  49,  99])\n",
      "labels: tensor([236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236,\n",
      "        236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236,\n",
      "        236, 236, 236, 236])\n",
      "predictions: tensor([ 71, 111, 158, 176,  12, 131, 147, 116, 153,  97, 107,  49, 176, 132,\n",
      "        149, 176,  75,  74, 100, 129,  61, 121,  84,  74,  21,  65,  99,  66,\n",
      "         96, 115, 148,  99])\n",
      "labels: tensor([236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 236,\n",
      "        236, 236, 236, 236, 236, 236, 236, 236, 236, 236, 237, 237, 237, 237,\n",
      "        237, 237, 237, 237])\n",
      "predictions: tensor([ 74,  17,  49, 115,  84, 153,  99,  99,  61,  42,  49, 149, 131, 132,\n",
      "         99, 167, 110, 153, 129, 129, 149, 153,  21,   4, 110, 110, 114,  67,\n",
      "         25,  75, 109, 141])\n",
      "labels: tensor([237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237,\n",
      "        237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237,\n",
      "        237, 237, 237, 237])\n",
      "predictions: tensor([110, 129, 115,   6,  38, 174, 114,   6,   4, 134,  28,  54,   8, 139,\n",
      "         15, 173,  43,  67, 155,   9, 138, 110, 110, 139,  40,   7, 148,  56,\n",
      "         52,  62,  68, 174])\n",
      "labels: tensor([237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237,\n",
      "        237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237,\n",
      "        237, 237, 237, 237])\n",
      "predictions: tensor([141, 110, 109, 115,  65,  96, 172,   7,  71,  60, 128,  84, 139,   6,\n",
      "        115,  39, 105, 115, 110,  43, 148, 110, 104,  49,  23, 160, 117, 178,\n",
      "        129,  43,   8, 119])\n",
      "labels: tensor([237, 237, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238,\n",
      "        238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238,\n",
      "        238, 238, 238, 238])\n",
      "predictions: tensor([110,  70,  46, 149, 161, 128, 143,  30, 161, 177,  69,  96, 170,  32,\n",
      "        151,  32,  26, 143, 149,  62,  17, 127, 109,  62, 127,  31, 162, 179,\n",
      "        173,  32,  21, 126])\n",
      "labels: tensor([238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238,\n",
      "        238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238, 238,\n",
      "        238, 238, 238, 238])\n",
      "predictions: tensor([101, 176, 164, 151, 122, 161, 164, 125,  89, 161, 125, 143, 161, 126,\n",
      "        125, 174,  68, 177,  32, 145,  52,  62, 126,  73, 170,  69,  64, 153,\n",
      "        128,  39,  61, 143])\n",
      "labels: tensor([238, 238, 238, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
      "        239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
      "        239, 239, 239, 239])\n",
      "predictions: tensor([143, 126,  30, 141,  66,  66,  38,  45,  38,  38, 139, 163, 178,  99,\n",
      "         99, 176,  99,  45, 114,  17,  99, 131,  99,  99,  66,  66,  45,  97,\n",
      "         99,  10,  17,  96])\n",
      "labels: tensor([239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
      "        239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
      "        239, 239, 239, 239])\n",
      "predictions: tensor([ 45, 155,  99,  25,  17,  10,  17,  99,  99,  66,   0, 178,  99,  51,\n",
      "         99,  99,  99, 141,   7, 155,  99,  97, 131, 163,   5,  79,  99,  67,\n",
      "        110,  43,  66,  99])\n",
      "labels: tensor([239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 240, 240,\n",
      "        240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "        240, 240, 240, 240])\n",
      "predictions: tensor([117, 110, 127, 163,  15,  38,  99, 109,  39,  73,  17,  17,  67,  36,\n",
      "        161, 141,  67, 103,  67, 108,  25,   6,  67,  25,  62, 143, 100, 138,\n",
      "         62,  45,  67,  36])\n",
      "labels: tensor([240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "        240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "        240, 240, 240, 240])\n",
      "predictions: tensor([174,  67, 167, 161,   6,  50,  58,  25, 141,  13,  67, 109,  95, 130,\n",
      "         66,  13,  23, 143, 138, 154,  55,  43, 143,  10,  37, 164, 109, 162,\n",
      "        164,  95,  67, 110])\n",
      "labels: tensor([240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "        240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "        240, 240, 240, 240])\n",
      "predictions: tensor([174,  32,  68, 171, 109,  30, 164,  99, 164,  45,  66,  15,  97,  67,\n",
      "        174, 164, 105,  23,  25,  67, 167,  37, 109,  67, 138, 105, 153, 100,\n",
      "         36,  67, 109, 164])\n",
      "labels: tensor([240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 241, 241, 241, 241,\n",
      "        241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241,\n",
      "        241, 241, 241, 241])\n",
      "predictions: tensor([ 58,  97, 138,  50, 138,  37,   6,  50,  64,  67,  95,  41, 141,  41,\n",
      "         99,  42, 163,  42,  42,  99,  99,  15,  41,  90,  67,  42,  99,  70,\n",
      "        159, 119,  15,  66])\n",
      "labels: tensor([241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241,\n",
      "        241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241,\n",
      "        241, 241, 241, 241])\n",
      "predictions: tensor([  9,  42,  42, 109,  32, 171, 163, 117,  99,   9,  42,  49,   9,  70,\n",
      "        163,  42, 178, 115,  99,  41,  42,  42,   9,  99,  99, 119, 115,  43,\n",
      "        114, 180,  99,  73])\n",
      "labels: tensor([241, 241, 241, 241, 241, 241, 241, 242, 242, 242, 242, 242, 242, 242,\n",
      "        242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
      "        242, 242, 242, 242])\n",
      "predictions: tensor([ 17,  42,  42, 115,  42,  26,   9, 128, 152, 126,  89,  24, 164,  68,\n",
      "        108,  89, 138,  72,  68, 108,  68, 125,  93, 109,  68,  89, 164,  17,\n",
      "        126, 109,  33,  72])\n",
      "labels: tensor([242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
      "        242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
      "        242, 242, 242, 242])\n",
      "predictions: tensor([126,  68, 126,  40,  89,  89, 125,   1,  89,  39, 126,  55, 161,  56,\n",
      "          1,  89, 108, 138,  46, 149, 138,  46,  89, 138,  68, 173, 126,  89,\n",
      "         46, 138,  46, 108])\n",
      "labels: tensor([242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242, 242,\n",
      "        242, 242, 242, 242, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243])\n",
      "predictions: tensor([ 68, 138,  12, 138, 108, 127,  68, 120,  96,  93,  46,  68,  68,  89,\n",
      "        126, 138, 125,  68,  63,  70,  92,  59,  72,  67, 131,  88,  15,  59,\n",
      "         59,   9,  49, 146])\n",
      "labels: tensor([243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243])\n",
      "predictions: tensor([144, 164,  72,  21,  42,  92, 128,  72,  57,  45, 126,  66,  99,   6,\n",
      "        131,  59,  59,  49,  87, 126,   6,  72, 118,  72,  59,  92,  11,  60,\n",
      "         72, 164,  45,  14])\n",
      "labels: tensor([243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 243, 243])\n",
      "predictions: tensor([ 83,  49, 122, 108,  24,   7,  89, 115,  11, 180, 118, 154,  99,   9,\n",
      "         36,  49,  72, 164,   7,   9,  99, 139,  84,  32,  17, 115,   6,  43,\n",
      "         59,  30,  34,  72])\n",
      "labels: tensor([243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243, 243,\n",
      "        243, 243, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244,\n",
      "        244, 244, 244, 244])\n",
      "predictions: tensor([ 42, 120,  99, 111, 160,  59,  45, 164, 180,  59,  72,   9, 164,  84,\n",
      "         68,  21,  63, 110,  31,  15,  14, 132,  32,  15,  98, 115,   9,  50,\n",
      "         15,   0,   9, 118])\n",
      "labels: tensor([244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244,\n",
      "        244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244,\n",
      "        244, 244, 244, 244])\n",
      "predictions: tensor([109,  68,  26,  27,  23,  52,  84,  15,  15,   0, 131, 110, 167,  77,\n",
      "         41,  15,  15,  14,   6,  14,  68,  99,  42, 149, 132, 132,  89,  17,\n",
      "        151, 115, 114,  37])\n",
      "labels: tensor([244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244,\n",
      "        244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244,\n",
      "        244, 244, 244, 244])\n",
      "predictions: tensor([153, 163, 139,  65,  99,  57,  32,  65, 100, 110, 126, 110,  98, 161,\n",
      "         17,  71,  42, 139, 178, 110,  17,  43, 110,  17,   9,  63, 117,  41,\n",
      "         63,  15,  14,  99])\n",
      "labels: tensor([244, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245,\n",
      "        245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245,\n",
      "        245, 245, 245, 245])\n",
      "predictions: tensor([ 92, 164,  68, 164, 108, 164, 143,  68, 108,  54, 127, 152,  67,  95,\n",
      "         89,  65, 152,  62, 127,  68, 143,  67,  68, 160, 164, 109,  71, 109,\n",
      "         67,  67, 122,  68])\n",
      "labels: tensor([245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245,\n",
      "        245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245, 245,\n",
      "        245, 245, 245, 245])\n",
      "predictions: tensor([146,   1,   1,  21, 164,  68,  67,  21, 164, 164, 164, 174, 138,  68,\n",
      "        164, 164, 164, 164, 169,  68, 164, 164,  67,  77, 152,  67,  67,  25,\n",
      "        155,  49,  67,  59])\n",
      "labels: tensor([245, 245, 245, 245, 245, 246, 246, 246, 246, 246, 246, 246, 246, 246,\n",
      "        246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246,\n",
      "        246, 246, 246, 246])\n",
      "predictions: tensor([ 67, 164, 131,  67,  67,  76,  21,  21,  21,  76,  75, 131, 154,  21,\n",
      "        131,  89,   9, 159, 131, 146, 131,  36,  15,  89,  74,  89,  76,  15,\n",
      "        161,  81,  15,  21])\n",
      "labels: tensor([246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246,\n",
      "        246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246,\n",
      "        246, 246, 246, 246])\n",
      "predictions: tensor([  9,  21,  89, 131,  21,   6,  12,  92, 131, 154, 131,  86,  89,  21,\n",
      "        131,  76, 109, 174, 146,  89, 160, 161,  76, 131,  89,  36,  21,  21,\n",
      "         21,  21,  15, 160])\n",
      "labels: tensor([246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246,\n",
      "        246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246, 246,\n",
      "        246, 246, 246, 246])\n",
      "predictions: tensor([ 21,  15,  76,  15,  21,  21,  21,  21, 131,  15,  21, 131, 131,  21,\n",
      "         15,  21,  76, 173, 131, 131, 146, 146,  21, 131,  15,  21,  21, 148,\n",
      "        161,  21,  21,  21])\n",
      "labels: tensor([246, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247,\n",
      "        247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247,\n",
      "        247, 247, 247, 247])\n",
      "predictions: tensor([146,   2, 148,  41,  92,  92, 149,  22,  83,  55,  15,  15, 162, 111,\n",
      "         97, 109,  70, 109,  66,  15,  15,  15, 149, 104,  71,  65,  27,  15,\n",
      "          6,  52, 167,  72])\n",
      "labels: tensor([247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247,\n",
      "        247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247,\n",
      "        247, 247, 247, 247])\n",
      "predictions: tensor([ 15,  15,  15,  40,  27, 107,  15, 163, 128,  22, 110,  27, 159,  75,\n",
      "         15, 128,  66,   6,  40, 128,  15, 135,  15,  63,  58,  71,  15,  41,\n",
      "         96,  15, 128, 128])\n",
      "labels: tensor([247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247,\n",
      "        247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247, 247,\n",
      "        247, 247, 247, 247])\n",
      "predictions: tensor([ 38,  70, 108, 171, 136,   2, 164,  15, 157, 121,  15,  15,  95,  77,\n",
      "         15,  15,  15, 128,  27,  81,  15,  85,  15,  97,  92,  15,  15,  70,\n",
      "         15,   9,  22,  65])\n",
      "labels: tensor([247, 247, 247, 247, 247, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
      "        248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
      "        248, 248, 248, 248])\n",
      "predictions: tensor([173,  15,  39,   4, 112, 128,  39,  10,  68,  50,  68, 109,  25, 141,\n",
      "        150,  96,  25, 162, 161,  95, 164,  97, 128, 143, 176, 152, 128,   6,\n",
      "         52, 176, 180,  82])\n",
      "labels: tensor([248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
      "        248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248,\n",
      "        248, 248, 248, 248])\n",
      "predictions: tensor([108, 153,  48,  48, 128,   2, 109, 126,  62, 143,  62,  25,  25,  46,\n",
      "        128,  48, 108, 164,  62,  66, 122,  17,  25,  49, 180,  46, 180,  10,\n",
      "        176, 176, 167,  46])\n",
      "labels: tensor([248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 248, 249, 249, 249,\n",
      "        249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249,\n",
      "        249, 249, 249, 249])\n",
      "predictions: tensor([ 25,  25, 176,  34, 177, 144,   0, 164,  62,  25,  62,  41,  61, 164,\n",
      "         91,  70,  91,  47,  91, 128,  91, 108, 108,  91,  92,  62, 177,   6,\n",
      "         91,  72, 177, 108])\n",
      "labels: tensor([249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249,\n",
      "        249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249,\n",
      "        249, 249, 249, 249])\n",
      "predictions: tensor([109,  19, 160,  89,  12, 138, 173, 138, 149,  67,  57,  72,  91, 164,\n",
      "         14,  57, 128, 171, 126, 133, 160,  91,  56,  46, 160,  64, 173, 128,\n",
      "         92,  48, 128,  72])\n",
      "labels: tensor([249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249,\n",
      "        249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249,\n",
      "        249, 249, 249, 249])\n",
      "predictions: tensor([ 50, 122,  89,  15,  70,  91,  92, 132,  91, 108,  21,  91, 143,  49,\n",
      "         56, 161, 128,  41, 177,  91, 177,  91, 177,  92, 108,  41, 164,  67,\n",
      "        109, 108,  56,  72])\n",
      "labels: tensor([249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 250,\n",
      "        250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250,\n",
      "        250, 250, 250, 250])\n",
      "predictions: tensor([128,  22,  89,  91,  39, 109,  91, 173, 177, 123,  15, 136,  45, 150,\n",
      "         62,  92,  75,  62,  25,  25,  91, 109,  25, 135,  70,  91, 177,  48,\n",
      "        127, 128,  39,  48])\n",
      "labels: tensor([250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250,\n",
      "        250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250,\n",
      "        250, 250, 250, 250])\n",
      "predictions: tensor([ 62, 109,  62,  62, 109, 161,  39,  62,  67,  62,  67,  57, 180, 122,\n",
      "         39,  53,  91,  32,  39,  62,  62, 127,  82,  62,  57, 176,  39,  91,\n",
      "         62,  48,  91,  91])\n",
      "labels: tensor([250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250,\n",
      "        250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 250,\n",
      "        250, 250, 250, 250])\n",
      "predictions: tensor([ 82, 107,  80,  89, 108,  39, 167,  74, 108, 128,  91,  62,  25,  91,\n",
      "         39,  39,  10, 153,  62,  10,  62,  62,  91, 141, 122, 176, 176,  39,\n",
      "         10, 177, 180,  74])\n",
      "labels: tensor([250, 250, 250, 250, 250, 250, 250, 250, 251, 251, 251, 251, 251, 251,\n",
      "        251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
      "        251, 251, 251, 251])\n",
      "predictions: tensor([153, 150, 176,  62, 122, 132,  93,  48,  22,  10, 109,  81,   4, 127,\n",
      "        108,  75, 153, 173,  92, 122,   4, 153, 161, 171,  32,   4,  60,  68,\n",
      "         62, 171,  25,  91])\n",
      "labels: tensor([251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
      "        251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
      "        251, 251, 251, 251])\n",
      "predictions: tensor([ 67, 108, 164, 109, 128,  75, 164, 109,  49,  91, 143,  33, 164, 108,\n",
      "        108,  68, 109, 132,  62, 126, 177, 172, 127, 176, 151, 127,  40,  89,\n",
      "        176, 108,  54,  39])\n",
      "labels: tensor([251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251,\n",
      "        251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 251, 252,\n",
      "        252, 252, 252, 252])\n",
      "predictions: tensor([172,  89,  30, 161,  49, 161, 109,  91,  67,  70, 122, 173, 172, 121,\n",
      "         39,  62, 174, 172, 109, 122, 164, 109,   1,  66,  67, 127,  59,  96,\n",
      "        109,  55,  55, 148])\n",
      "labels: tensor([252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
      "        252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
      "        252, 252, 252, 252])\n",
      "predictions: tensor([ 42, 101, 136,  73,  55,  69,  87, 136,  69, 119,  85,  69, 154,  64,\n",
      "         73, 147,  28,  96,  28,  26,  28, 136,  69, 166,  83,  83,  28,  69,\n",
      "        148,  17,  12,  54])\n",
      "labels: tensor([252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
      "        252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252, 252,\n",
      "        252, 252, 252, 252])\n",
      "predictions: tensor([ 67,  30,  30,  42, 117,  28, 130, 119,  55,  78,  68,  69,  97,  30,\n",
      "         87,  28,  55, 158,  69, 101,  28,  54,  32,  55,  55,  78, 101,  96,\n",
      "         28,  55,  37,  60])\n",
      "labels: tensor([252, 252, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253,\n",
      "        253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253,\n",
      "        253, 253, 253, 253])\n",
      "predictions: tensor([ 42, 149,  39,  48, 151,  91, 122,  74, 176,  39,  15,  62,  48,  57,\n",
      "         62,  12,  39, 180, 108, 131, 102,  39,  91,  91, 177, 153,  39,  67,\n",
      "        127,  62, 126, 122])\n",
      "labels: tensor([253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253,\n",
      "        253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253, 253,\n",
      "        253, 253, 253, 253])\n",
      "predictions: tensor([176, 102,  39,  39, 128, 122,  91,  91, 177,  73,  91,  89, 176, 176,\n",
      "         52,  91, 164,  35,  62,  91, 161,   1,   9,  39,  39, 167,  91,  48,\n",
      "         78, 122, 127,  39])\n",
      "labels: tensor([254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254,\n",
      "        254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254,\n",
      "        254, 254, 254, 254])\n",
      "predictions: tensor([173,  11,  15, 153,  70,  24, 128,  15,  15,  89, 169,  36,  15,  66,\n",
      "         15, 128,  99,  70,  81,  71,  24,  66,  10,  92,  72, 110, 160,  30,\n",
      "         15, 128,  15,  14])\n",
      "labels: tensor([254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254,\n",
      "        254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254,\n",
      "        254, 254, 254, 254])\n",
      "predictions: tensor([ 39, 131,  70, 136,  99, 119,  40,  97,  66, 131,  15,  15,  72, 173,\n",
      "         39, 127,  71,  17,  15,   7,  43, 170,  63,  39, 176,  96,  72,  96,\n",
      "         39, 128,  14,  92])\n",
      "labels: tensor([254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254,\n",
      "        254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254,\n",
      "        254, 254, 254, 254])\n",
      "predictions: tensor([ 15, 148, 131, 106,  15,  15,   9,  72,  70,  93,  96,  15,  72, 108,\n",
      "        173,  99,  67,  67,  42,  85, 167,  39,  17,  23,  17,  15,  39,  97,\n",
      "         14, 159,  96, 118])\n",
      "labels: tensor([254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254,\n",
      "        254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 254, 255, 255,\n",
      "        255, 255, 255, 255])\n",
      "predictions: tensor([131,  15,  15,  17,  15,  92,  99,  89,  99,  70, 148,  17,  65,  14,\n",
      "        173,  15, 131,  15,  15,  15,  92,  76, 148,  39, 104, 129, 149, 152,\n",
      "         73,  89,  64,  64])\n",
      "labels: tensor([255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "        255, 255, 255, 255])\n",
      "predictions: tensor([ 30, 108, 149, 149,  17,  64, 180, 108,  64, 154, 164, 154,   1,  32,\n",
      "        149,  49,  73, 154,  30,  30,  64,  68, 107,  30, 132, 154,  99, 154,\n",
      "         62, 128,  30,  30])\n",
      "labels: tensor([255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "        255, 255, 255, 255])\n",
      "predictions: tensor([164,  50,  93, 161,  30,   7,   2,  55, 141, 177, 164, 164,  30,  28,\n",
      "        154,  52, 132, 164,   7,  26, 107, 154, 149,  66,  40,  64,  73,   2,\n",
      "        161, 154,  68, 157])\n",
      "labels: tensor([255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\n",
      "        255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 256, 256, 256,\n",
      "        256, 256, 256, 256])\n",
      "predictions: tensor([149, 148,  33, 173,  49,  49, 180, 119, 131,  93, 164, 147,  36, 161,\n",
      "         89, 161,  89,  14, 164,  30, 159, 174, 131,  73, 154,  43, 128,  91,\n",
      "        126, 127, 153,  68])\n",
      "labels: tensor([256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
      "        256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
      "        256, 256, 256, 256])\n",
      "predictions: tensor([ 68, 126, 172,  24, 136, 139, 138, 126,  22,  56, 122, 122, 132, 177,\n",
      "         89,  62, 128, 127, 128, 125,  68, 138,   5, 122,   4, 154,   4,  53,\n",
      "         50,  31, 127, 126])\n",
      "labels: tensor([256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
      "        256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
      "        256, 256, 256, 256])\n",
      "predictions: tensor([  1, 126,  62,  22, 134, 108,  75, 166, 164,  32, 108,  10,  81,  81,\n",
      "         61,  56, 136,  91, 138,  93,  75, 128,  91, 172,  56,  62, 173,  22,\n",
      "         62,  62,  22, 148])\n",
      "labels: tensor([256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
      "        256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256,\n",
      "        257, 257, 257, 257])\n",
      "predictions: tensor([ 89,  70,  15, 128,  93,  42,  89,  75, 126, 164,  53,  56, 117,  22,\n",
      "         48,  89, 127, 126,  81, 127,  68, 135, 174, 126,  89, 135,  75,  78,\n",
      "         15,  90,  15,  15])\n",
      "labels: tensor([257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257,\n",
      "        257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257,\n",
      "        257, 257, 257, 257])\n",
      "predictions: tensor([136,  42, 154,  15,   2,  34, 173,  15,  15,  92,  15,   1,  91,   2,\n",
      "         15, 136, 110, 119,  15,   2,  90, 143, 128,  92, 119, 136, 173, 103,\n",
      "         45,  12, 136,  15])\n",
      "labels: tensor([257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257,\n",
      "        257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257,\n",
      "        257, 257, 257, 257])\n",
      "predictions: tensor([ 15,  32,  68,  90,  15,  97, 170,  15, 107,  15,  15,  39,  58,  15,\n",
      "         15,  15,  15,  15,  92,  15,  15,  31,  15,  97,  91,   0,  97,  63,\n",
      "         15,   1, 148,  63])\n",
      "labels: tensor([257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257,\n",
      "        257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257,\n",
      "        257, 257, 257, 257])\n",
      "predictions: tensor([ 99,  73,  42, 154,  99,  63,  17, 171, 110, 128,   7,  15,  15, 119,\n",
      "          1, 135,  15,  15, 119, 119,  82, 135, 110,  15,  58,  63, 154,  15,\n",
      "          2,  59,  22, 135])\n",
      "labels: tensor([257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 257, 258, 258, 258,\n",
      "        258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258,\n",
      "        258, 258, 258, 258])\n",
      "predictions: tensor([ 15,  50,  82,  99,  73, 118, 136,  42, 171,  42,  15, 122,  25, 127,\n",
      "        135,  95,  62,  64,  62, 134, 164, 164,  58,  13,  77, 164,  39,  62,\n",
      "         67,  26, 152, 122])\n",
      "labels: tensor([258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258,\n",
      "        258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258,\n",
      "        258, 258, 258, 258])\n",
      "predictions: tensor([ 54, 149, 156, 164, 119, 127, 173, 125, 164, 118, 164, 167,  67,  25,\n",
      "         95,  32,  75, 127, 108, 122,  13, 122, 152,  64, 164, 153,  96,  12,\n",
      "         61,  32,  25,  25])\n",
      "labels: tensor([258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258,\n",
      "        258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258,\n",
      "        258, 258, 258, 258])\n",
      "predictions: tensor([ 67,  25, 149, 164, 164, 164,  25,  13,  25,  87, 108, 164, 127, 127,\n",
      "         73,  32,  17,  62,  73,  62,  39, 127, 164,  39, 158,  67, 127, 174,\n",
      "         95, 143,  67,  96])\n",
      "labels: tensor([258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258, 258,\n",
      "        258, 258, 258, 258, 258, 259, 259, 259, 259, 259, 259, 259, 259, 259,\n",
      "        259, 259, 259, 259])\n",
      "predictions: tensor([ 68,  97, 168, 127, 132,  44,  56,  39, 122,  25, 135,  96,  77,  68,\n",
      "         75, 155,  32,  49,  45,  15,  15, 138, 166,  39, 177,  15,  71,  15,\n",
      "         15, 128,  39,  92])\n",
      "labels: tensor([259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259,\n",
      "        259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259,\n",
      "        259, 259, 259, 259])\n",
      "predictions: tensor([ 15, 138,  15,  15,  39,  92,  15,  15,  15,  63, 148,  15,  15,  70,\n",
      "         82, 177,  91,  22,  15,  92,  48,  22, 109,  39,  92,  15,  15, 119,\n",
      "         68,  15, 166, 128])\n",
      "labels: tensor([259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259,\n",
      "        259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259, 259,\n",
      "        259, 259, 259, 259])\n",
      "predictions: tensor([ 39,  92, 153, 112, 173,  97, 109,  15,  81,  70,  70,  15, 148, 136,\n",
      "        138, 136,  39, 174, 138, 167,  15, 177,  39,  92,  22, 177,  39, 138,\n",
      "         70, 137,   5, 151])\n",
      "labels: tensor([259, 259, 259, 259, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260,\n",
      "        260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260,\n",
      "        260, 260, 260, 260])\n",
      "predictions: tensor([ 97, 166,  52, 121,  68,  21,  49,  12,  68, 127,  75, 147,  30,  77,\n",
      "        169,  12, 164,  68,   1,  30, 127, 128, 164,  68,   1,  89, 101, 179,\n",
      "         89, 108, 127, 127])\n",
      "labels: tensor([260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260,\n",
      "        260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260,\n",
      "        260, 260, 260, 260])\n",
      "predictions: tensor([ 72,  77, 131,  30,  68,  72, 167,  21,  49,  68,  89, 146, 177, 173,\n",
      "         12, 108,  21,  21,  12, 127,  89, 127, 167, 152,  91,  30, 172,  91,\n",
      "          1, 132, 148,  51])\n",
      "labels: tensor([260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260, 260,\n",
      "        260, 260, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "        261, 261, 261, 261])\n",
      "predictions: tensor([ 70,  21,  21,  72,  89,  89, 101, 164,  39, 127, 126, 134,  68, 110,\n",
      "         30, 127,  30, 149,  72,  21,  21,  72,  38,   4,  68, 173,  15,  39,\n",
      "         96,  48, 146,  92])\n",
      "labels: tensor([261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "        261, 261, 261, 261])\n",
      "predictions: tensor([132, 132,  74, 166,  89, 170, 128, 128, 149,   1,  97,  89, 127,  30,\n",
      "         46, 173,  39, 180,   4,  92,  91, 126,  36,  72,  50, 153,  89, 177,\n",
      "          4,  15, 161,  68])\n",
      "labels: tensor([261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "        261, 261, 261, 261])\n",
      "predictions: tensor([151, 150,  15,  91,  75, 128,  96,  39,  15, 162,  62, 176,   1, 128,\n",
      "        118,   1,  15,  38,  97,  48, 119,  91, 127,  62,  15,  22,  91,  10,\n",
      "         62,  39,  91,  22])\n",
      "labels: tensor([261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
      "        261, 261, 261, 261, 261, 261, 262, 262, 262, 262, 262, 262, 262, 262,\n",
      "        262, 262, 262, 262])\n",
      "predictions: tensor([ 62, 128,  48,  75, 180, 112,   0,  65, 128,  70,  39, 177,   1,  15,\n",
      "        145,  62, 177,  96,  24,  56,  41,  96,  89,  66,  10, 110,  15, 115,\n",
      "        110,  63,  64,  50])\n",
      "labels: tensor([262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262,\n",
      "        262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262,\n",
      "        262, 262, 262, 262])\n",
      "predictions: tensor([ 99, 110,  42,   8, 119, 108,  55,  99, 159,  42, 155,  40,  38,  31,\n",
      "        149,  66,   2,  71,  68, 111,  77,  66,  40,  42, 108,  89, 148,  66,\n",
      "         91,  15,  21,  30])\n",
      "labels: tensor([262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262, 262,\n",
      "        263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
      "        263, 263, 263, 263])\n",
      "predictions: tensor([164,  86,  63,   8,  65,  23,  66, 149,  62,  10,  40,  65, 155,  66,\n",
      "         99, 110,  84, 110,  43,   9, 163,   8,  63, 110,  63,  63,   6, 104,\n",
      "        110, 109,   0, 115])\n",
      "labels: tensor([263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
      "        263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
      "        263, 263, 263, 263])\n",
      "predictions: tensor([  6,  99,   9, 110,  66, 115,  84, 163, 110,  14, 115, 115,  84,  84,\n",
      "        110, 110,   8, 110, 110,  98, 155, 163,  31, 110,   6,  84, 115, 141,\n",
      "        110,  84,  84,  99])\n",
      "labels: tensor([263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
      "        263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
      "        263, 263, 263, 263])\n",
      "predictions: tensor([ 63,   9,  31,   8,  15, 110,  99,  99, 110,  63,  16, 109, 163,  99,\n",
      "         99,  41, 158,  98,  63, 110, 115,  63,  84, 159, 155,  98, 155,  27,\n",
      "         31,   9, 115,  17])\n",
      "labels: tensor([263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263,\n",
      "        263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 263, 264,\n",
      "        264, 264, 264, 264])\n",
      "predictions: tensor([ 98, 104,  98,  16, 110, 110,  84,   6, 114, 114,   8,  54,  16,  63,\n",
      "          6, 110, 115,  95,  63,  15,  99,  27, 109, 115,  98,   9,   6,   4,\n",
      "        154,   5, 161,  32])\n",
      "labels: tensor([264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264,\n",
      "        264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264,\n",
      "        264, 264, 264, 264])\n",
      "predictions: tensor([164, 174,   2, 174,  68,  26,   5, 139, 122,  63, 109, 154,  10, 166,\n",
      "        141, 155, 164, 152, 118,  49,  67,  30, 115,   2, 164,  97,   5, 145,\n",
      "         69,  69,  87,  29])\n",
      "labels: tensor([264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264,\n",
      "        264, 264, 264, 264, 264, 264, 264, 264, 264, 264, 264])\n",
      "predictions: tensor([ 26,  32,  63,  26, 143,  32,  32, 104,  99,  19,  25,  58,   2, 143,\n",
      "        164, 162, 139,  97, 139, 108, 104, 164,  26,  17,   5])\n",
      "Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            #print(f'outputs: {outputs}')\n",
    "            #print(f'labels: {labels}')\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            #print(f'predictions: {predicted}')\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = evaluate(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Learning Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: AlexNet as a Fixed Feature Extractor\n",
    "Extract the values of the activations of AlexNet on the face images. Use those as features in order to perform face classification: learn a fully-connected neural network that takes in the activations of the units in the AlexNet layer as inputs, and outputs the name of the person. Below, include a description of the system you built and its performance. It is recommended to start out with only using the `conv4` activations. Using `conv4` is sufficient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Tolga/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "c:\\Users\\Tolga\\anaconda3\\envs\\comp541\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "alexnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(inplace=True)\n",
      "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(alexnet.features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "output = alexnet.features[:10](dummy_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedAlexNet, self).__init__()\n",
    "        self.alexnet = torch.nn.Sequential(*list(alexnet.features)[:10])\n",
    "        self.fc1 = nn.Linear(256 * 13 * 13, 300)  # First hidden layer (300 units)\n",
    "        self.fc2 = nn.Linear(300, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alexnet(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 28x28 pixels\n",
    "    transforms.ToTensor(),  # Convert image to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize to ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'actor_faces'\n",
    "\n",
    "dataset = ActorFacesDataset(root_dir=root_dir, transform=transform)\n",
    "\n",
    "train_images, testval_images = train_test_split(dataset.image_paths, test_size=0.3, random_state=42)\n",
    "val_images, test_images = train_test_split(testval_images, test_size=0.5, random_state=42)\n",
    "\n",
    "train_labels = [dataset.labels[dataset.image_paths.index(img)] for img in train_images]\n",
    "val_labels = [dataset.labels[dataset.image_paths.index(img)] for img in val_images]\n",
    "test_labels = [dataset.labels[dataset.image_paths.index(img)] for img in test_images]\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, range(len(train_images)))\n",
    "val_dataset = torch.utils.data.Subset(dataset, range(len(train_images), len(train_images) + len(val_images)))\n",
    "test_dataset = torch.utils.data.Subset(dataset, range(len(train_images) + len(val_images), len(dataset)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Train Loss: 5.7942, Validation Loss: 5.7759\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "model = ModifiedAlexNet(dataset.class_num)\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            #print(f'outputs: {outputs.shape}')\n",
    "            #print(f'labels: {labels.shape}')\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation Loss\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "\n",
    "                #print(f'outputs: {outputs.shape}')\n",
    "                #print(f'labels: {labels.shape}')\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train the model\n",
    "epochs = 1\n",
    "train_losses, val_losses = train(model, train_loader, val_loader, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fc_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\"\"\"\n",
    "# Test set\n",
    "X_test, y_test = ...  # Split features and labels for testing\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_fc_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3: Visualize Weights\n",
    "Train two networks the way you did in Part 2.1. Use 300 and 800 hidden units in the hidden layer. Visualize 2 different hidden features (neurons) for each of the two settings, and briefly explain why they are interesting. A sample visualization of a hidden feature is shown below. Note that you probably need to use L2 regularization while training to obtain nice weight visualizations.\n",
    "\n",
    "![](figures/figure2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4: Finetuning AlexNet\n",
    "Train two networks the way you did in Part 2.1. Use 300 and 800 hidden units in the hidden layer. Visualize 2 different hidden features (neurons) for each of the two settings, and briefly explain why they are interesting. A sample visualization of a hidden feature is shown in Figure 4. Note that you probably need to use L2 regularization while training to obtain nice weight visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.5: Bonus: Gradient Visualization\n",
    "Here, you will use [Utku Ozbulak’s PyTorch CNN Visualizations Library](https://github.com/utkuozbulak/pytorch-cnn-visualizations/) to visualize the important parts of the input image for a particular output class. In particular, just select a specific picture of an actor, and then using your trained network in Part 2.4, perform Gradient visualization with guided backpropagation to understand the prediction for that actor with respect to the input image. Comment on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Turn In\n",
    "You have two options for submission:\n",
    "1) Provide all the relevant answers to questions, images, figures, etc, in this Jupyter notebook, convert the jupyter notebook into a PDF, and upload the PDF.\n",
    "2) Write all the answers to the questions and any relevant figures in a LaTeX report, convert the report to a PDF, and upload a zip file containing both the jupyter notebook and the report. \n",
    "\n",
    "## Grading\n",
    "The assignment will be graded out of `100` points: `0` (no submission), `20` (an attempt at a solution), `40` (a partially correct solution), `60` (a mostly correct solution), `80` (a correct solution), `100` (a particularly creative or insightful solution). The grading depends on both the content and clarity of your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp541",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
