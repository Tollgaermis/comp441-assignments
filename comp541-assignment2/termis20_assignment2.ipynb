{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Convolutional Neural Networks\n",
    "Instructions: In Assignment 2, you will learn all about the convolutional neural networks. In particular, you will gain a first-hand experience of the training process, understand the architectural details, and familiarize with transfer learning\n",
    "with deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Convolutional Neural Networks\n",
    "In this part, you will experiment with a convolutional neural network implementation to perform image classification. The dataset we will use for this assignment was created by Zoya Bylinskii, and contains 451 works of art from 11 different artists all downsampled and padded to the same size. The task is to identify which artist produced each image. The original images can be found in the `art_data/artists` directory included with the data zip file. The composition of the dataset and a sample painting from each artist are shown in Table 1.\n",
    "\n",
    "Figure 1 shows an example of the type of convolutional architecture typically employed for similar image recognition problems. Convolutional layers apply filters to the image, and produce layers of\n",
    "feature maps. Often, the convolutional layers are interspersed with pooling layers. The final layers of the network are fully connected, and lead to an output layer with one node for each of the K classes\n",
    "the network is trying to detect. We will use a similar architecture for our network.\n",
    "\n",
    "![](figures/figure1.jpg)\n",
    "\n",
    "The code for performing the data processing and training the network is provided in the starter\n",
    "pack. You will use PyTorch to implement convolutional neural networks. We create a dataset from the artists’ images by downsampling them to 50x50 pixels, and transforming the RGB values to lie within the range $[-0.5, 0.5]$. We provide a lot of starter code below, but you will need to modify the hyperparameters and network structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1: Convolutional Filter Receptive Field\n",
    "\n",
    "First, it is important to develop an intuition for how a convolutional layer affects the feature representations that the network learns. Assume that you have a network in which the first convolutional layer\n",
    "applies a 5x5 patch to the image, producing a feature map $Z_{1}$. The next layer of the network is also convolutional; in this case, a 3x3 patch is applied to the feature map $Z_{1}$ to produce a new feature\n",
    "map, $Z_{2}$. Assume the stride used in both cases is 1. Let the receptive field of a node in this network be the portion of the original image that contributes information to the node (that it can, through the filters of the network, “see”). What are the dimensions of the receptive field for a node in $Z_{2}$? Note that you can ignore padding, and just consider patches in the middle of the image and $Z_{1}$. Thinking about your answer, why is it effective to build convolutional networks deeper, i.e. with more layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**<br>\n",
    "For a node in $Z_{2}$ the receptive field is 7x7. A 7x7 receptive field can be accomplished directly by using a 7x7 filter. But if we do it as explained above or even better with 3 3x3 filters, we will use more non-linear layers and the extracted features will be improved.<br> Also, using a single 7x7 layer, we will need K x (7 x 7 x C) = 49 x K x C parameters where K is the number of filters (or output channels) and C is the number of input channels. Whereas using 3 3x3 filters we will need  3 x (K x (3 x 3 x C)) = 27 x K x C for the same output volume, less that a single convoluional layer with a 7x7 filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Run the PyTorch ConvNet\n",
    "\n",
    "Study the provided SimpleCNN class below, and take a look at the hyperparameters. Answer the following questions about the initial implementation:\n",
    "\n",
    "1) How many layers are there? Are they all convolutional? If not, what structure do they have?\n",
    "2) Which activation function is used on the hidden nodes?\n",
    "3) What loss function is being used to train the network?\n",
    "4) How is the loss being minimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**<br\n",
    "The architecture:<br>\n",
    "CONV -> ReLU -> POOL (?) -> CONV -> RelLU -> ReLU -> POOL (?) -> FC -> ReLU -> FC<br>\n",
    "There are 2 convolutional layers, which are both followed by ReLU activation functions. Depending on the pooling flag, these are followed by pooling. At the end, there are 2 fully connected layers and a ReLU activation function in between.<br>\n",
    "Cross entropy loss is being used in this multiclass classification problem.<br>\n",
    "The loss is being minimized by backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are familiar with the code, try training the network. It should take between 60-120 seconds to train for 50 epochs. What is the training accuracy for your network after training? What is the validation accuracy? What do these two numbers tell you about what your network is doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageFile\n",
    "import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import time\n",
    "import random\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(torch.nn.Module):\n",
    "    def __init__(self,device,pooling= False):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.pooling = pooling\n",
    "        self.conv_layer1 =  torch.nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=2, device=device)\n",
    "        self.pool_layer1 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv_layer2 = torch.nn.Conv2d(in_channels=16,out_channels=16,kernel_size=5,stride=2, device=device)\n",
    "        self.pool_layer2 = torch.nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        if pooling:\n",
    "            self.fully_connected_layer = nn.Linear(64,64, device=device)\n",
    "            self.final_layer = nn.Linear(64,11, device=device)\n",
    "        else:\n",
    "            self.fully_connected_layer = nn.Linear(1600, 64, device=device)\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "            self.final_layer = nn.Linear(64, 11, device=device)\n",
    "    def forward(self,inp):\n",
    "        x = torch.nn.functional.relu(self.conv_layer1(inp))\n",
    "        if self.pooling:\n",
    "            x = self.pool_layer1(x)\n",
    "        x = torch.nn.functional.relu(self.conv_layer2(x))\n",
    "        if self.pooling:\n",
    "            x = self.pool_layer2(x)\n",
    "        x = x.reshape(x.size(0),-1)\n",
    "        x = torch.nn.functional.relu(self.fully_connected_layer(x))\n",
    "        #x = self.dropout(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderClass(Dataset):\n",
    "    def __init__(self,data,labels,phase,transforms):\n",
    "        super(LoaderClass, self).__init__()\n",
    "        self.transforms = transforms\n",
    "        self.labels = labels[phase + \"_labels\"]\n",
    "        self.data = data[phase + \"_data\"]\n",
    "        self.phase = phase\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img = self.data[idx]\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transforms(img)\n",
    "        return img,torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,model,criterion,tr_loader,val_loader,optimizer,\n",
    "                 num_epoch,patience,batch_size,lr_scheduler=None):\n",
    "        self.model = model\n",
    "        self.tr_loader = tr_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.num_epoch = num_epoch\n",
    "        self.patience = patience\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.criterion = criterion\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.no_inc = 0\n",
    "        self.best_loss = 9999\n",
    "        self.phases = [\"train\",\"val\"]\n",
    "        self.best_model = []\n",
    "        self.best_val_acc = 0\n",
    "        self.best_train_acc = 0\n",
    "        self.best_val_loss = 0\n",
    "        self.best_train_loss = 0\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        pass\n",
    "    def train(self):\n",
    "        pbar = tqdm.tqdm(desc= \"Epoch 0, phase: Train\",postfix=\"train_loss : ?, train_acc: ?\")\n",
    "        for i in range(self.num_epoch):\n",
    "            last_train_acc = 0\n",
    "            last_val_acc = 0\n",
    "            last_val_loss = 0\n",
    "            last_train_loss = 0\n",
    "            pbar.update(1)\n",
    "\n",
    "            for phase in self.phases:\n",
    "                total_acc = 0\n",
    "                total_loss = 0\n",
    "                start = time.time()\n",
    "                if phase == \"train\":\n",
    "                    pbar.set_description_str(\"Epoch %d,\"% i + \"phase: Training\")\n",
    "                    loader = self.tr_loader\n",
    "                    self.model.train()\n",
    "                else:\n",
    "                    pbar.set_description_str(\"Epoch %d,\"% i + \"phase: Validation\")\n",
    "                    loader = self.val_loader\n",
    "                    self.model.eval()\n",
    "                iter = 0\n",
    "                for images,labels in loader:\n",
    "                    iter += 1\n",
    "                    images = images.to(self.model.device)\n",
    "                    labels = labels.to(self.model.device)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    logits = self.model(images)\n",
    "                    softmaxed_scores = self.softmax(logits)\n",
    "                    _, predictions = torch.max(softmaxed_scores,1)\n",
    "                    _, labels = torch.max(labels,1)\n",
    "                    loss = self.criterion(softmaxed_scores.float(),labels.long())\n",
    "                    total_loss += loss.item()\n",
    "                    total_acc += torch.sum(predictions == labels).item()\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        pbar.set_postfix_str(\"train acc: %6.3f,\" %(total_acc/ (iter*self.batch_size)) + (\"train loss: %6.3f\" % (total_loss / iter)))\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "                    else:\n",
    "                        pbar.set_postfix_str(\"val acc: %6.3f,\" %(total_acc/ (iter*self.batch_size)) + (\"val loss: %6.3f\" % (total_loss / iter)))\n",
    "\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    if self.lr_scheduler:\n",
    "\n",
    "                        self.lr_scheduler.step()\n",
    "                end = time.time()\n",
    "                if phase == \"train\":\n",
    "                    loss_p = total_loss / iter\n",
    "                    acc_p = total_acc / len(self.tr_loader.dataset)\n",
    "                    last_train_acc = acc_p\n",
    "                    last_train_loss = loss_p\n",
    "                else:\n",
    "                    loss_p = total_loss / iter\n",
    "                    acc_p = total_acc / len(self.val_loader.dataset)\n",
    "                    last_val_acc = acc_p\n",
    "                    last_val_loss = loss_p\n",
    "\n",
    "                    if loss_p < self.best_loss:\n",
    "                        print(\"New best loss, loss is: \",str(loss_p), \"acc is: \",acc_p )\n",
    "                        self.best_loss = loss_p\n",
    "                        self.no_inc = 0\n",
    "                        self.best_model = self.model\n",
    "                        self.best_train_acc = last_train_acc\n",
    "                        self.best_train_loss = last_train_loss\n",
    "                        self.best_val_loss = last_val_loss\n",
    "                        self.best_val_acc = last_val_acc\n",
    "                    else:\n",
    "                        print(\"Not a better score\")\n",
    "\n",
    "\n",
    "                        self.no_inc += 1\n",
    "                        if self.no_inc == self.patience:\n",
    "                            print(\"Out of patience returning the best model\")\n",
    "                            print(\n",
    "                                \"Best val acc: {}, Best val loss: {}, Best train acc: {}, Best train loss: {} \".format(\n",
    "                                    self.best_val_acc, self.best_val_loss, self.best_train_acc, self.best_train_loss\n",
    "                                ))  # Stats of the best model\n",
    "                            return self.best_model\n",
    "        print(\"Training ended returning the best model\")\n",
    "        print(\n",
    "            \"Best val acc: {}, Best val loss: {}, Best train acc: {}, Best train loss: {} \".format(\n",
    "                self.best_val_acc, self.best_val_loss, self.best_train_acc, self.best_train_loss\n",
    "            ))  # Stats of the best model\n",
    "        return self.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "Momentum = 0.9 # If you use SGD with momentum\n",
    "BATCH_SIZE = 16\n",
    "POOLING = False\n",
    "NUM_EPOCHS = 200\n",
    "PATIENCE = -1\n",
    "TRAIN_PERCENT = 0.8\n",
    "VAL_PERCENT = 0.2\n",
    "NUM_ARTISTS = 11\n",
    "DATA_PATH = \"./art_data/artists\"\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True # Do not change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artist_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    artists = [x for x in os.listdir(DATA_PATH) if x != '.DS_Store']\n",
    "    print(artists)\n",
    "    for folder in os.listdir(DATA_PATH):\n",
    "        class_index = artists.index(folder)\n",
    "        for image_name in os.listdir(DATA_PATH + \"/\" + folder):\n",
    "            img = Image.open(DATA_PATH + \"/\" + folder + \"/\" + image_name)\n",
    "            artist_label = (np.arange(NUM_ARTISTS) == class_index).astype(np.float32)\n",
    "            data.append(np.array(img))\n",
    "            labels.append(artist_label)\n",
    "    shuffler = np.random.permutation(len(labels))\n",
    "    data = np.array(data)[shuffler]\n",
    "    labels = np.array(labels)[shuffler]\n",
    "\n",
    "    length = len(data)\n",
    "    val_size = int(length*0.2)\n",
    "    val_data = data[0:val_size+1]\n",
    "    train_data = data[val_size+1::]\n",
    "    val_labels = labels[0:val_size+1]\n",
    "    train_labels = labels[val_size+1::]\n",
    "    print(val_labels)\n",
    "    data_dict = {\"train_data\":train_data,\"val_data\":val_data}\n",
    "    label_dict = {\"train_labels\":np.array(train_labels),\"val_labels\":np.array(val_labels)}\n",
    "\n",
    "    return data_dict,label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "data,labels = load_artist_data()\n",
    "model = SimpleCNN(device=device,pooling=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(50),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LoaderClass(data,labels,\"train\",transform[\"train\"])\n",
    "valid_dataset = LoaderClass(data,labels,\"val\",transform[\"val\"])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             shuffle=True, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard adam optimizer, no weight decay, no dropout, no scheduler\n",
    "criterion = CrossEntropyLoss()\n",
    "trainer_m = Trainer(model, criterion, train_loader, val_loader, optimizer, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model = trainer_m.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: Add Pooling Layers\n",
    "We will now add max pooling layers after each of our convolutional layers. This code has already been provided for you; all you need to do is switch the pooling flag in the hyper-parameters to True,\n",
    "and choose different values for the pooling filter size and stride. After you applied max pooling, what happened to your results? How did the training accuracy vs. validation accuracy change? What does\n",
    "that tell you about the effect of max pooling on your network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new model because the other model's params got updated.\n",
    "model_pooling = SimpleCNN(device=device,pooling=True)\n",
    "trainer_m_pooling = Trainer(model_pooling, criterion, train_loader, val_loader, optimizer, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_pooling = trainer_m_pooling.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**<br>\n",
    "Applying pooling reduces the spatial dimensions, helping the model to focus on the most important features. This also helps with avoiding overfitting as the number of parameters are decreased.<br><br>\n",
    "Without pooling:<br>\n",
    "Best val acc: 0.4835164835164835, Best val loss: 2.047486503918966, Best train acc: 0.6722222222222223, Best train loss: 1.8831380450207253<br><br>\n",
    "With max pooling:<br>\n",
    "Best val acc: 0.4945054945054945, Best val loss: 2.036665161450704, Best train acc: 0.8277777777777777, Best train loss: 1.718546924383744<br><br>\n",
    "The training accuracy improved significantly. There was also a slight improvement in the validation accuracy.<br>\n",
    "The model with max pooling has increased accuracy but struggles to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.4: Regularize Your Network!\n",
    "Because this is such a small dataset, your network is likely to overfit the data. Implement the following ways of regularizing your network. Test each one individually, and discuss how it affects your results.\n",
    "\n",
    "- __Dropout__: In PyTorch, this is implemented using the `torch.nn.dropout` class, which takes a value called the `keep_prob`, representing the probability that an activation will be dropped out. This value should be between 0.1 and 0.5 during training, and 0 for evaluation and testing. An example of how this works is available here. You should add this to your network and try different values to find one that works well.\n",
    "\n",
    "- __Weight Regularization__: You should try different optimizers, and different weight decay values for optimizers.\n",
    "\n",
    "- __Early Stopping__: Stop training your model after your validation accuracy starts to plateau or decrease (so you do not overtrain your model). The number of steps can be controlled through the `patience` hyperparameter in the code.\n",
    "\n",
    "- __Learning Rate Scheduling__: Learning rate scheduling is an important part of training neural networks. There are a lot of techniques for learning rate scheduling. You should try\n",
    "different schedulers such as `StepLR`, `CosineAnnealing`, etc.\n",
    "\n",
    "Give your results for each of these regularization techniques, and discuss which ones were the most effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**<br>\n",
    "Testing each individually:<br><br>\n",
    "Dropout (p=0.5) between the two fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the dropout in forward function of the network declaration\n",
    "model_dropout = SimpleCNN(device=device,pooling=True)\n",
    "trainer_m_dropout = Trainer(model_dropout, criterion, train_loader, val_loader, optimizer, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_dropout = trainer_m_dropout.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the training accuracy is similar to no dropout, the validation accuracy increased significantly meaning that the model generalizes well to new data.<br>\n",
    "<br>\n",
    "Weight regularization:<br>\n",
    "(commenting out the dropout in CNN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = SimpleCNN(device=device,pooling=True)\n",
    "opt_sgd = torch.optim.SGD(model_sgd.parameters(), lr=LR, weight_decay=0.0005)\n",
    "trainer_m_sgd = Trainer(model_sgd, criterion, train_loader, val_loader, opt_sgd, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_sgd= trainer_m_sgd.train()\n",
    "# Best val acc: 0.07692307692307693, Best val loss: 2.398476759592692, Best train acc: 0.07777777777777778, Best train loss: 2.3980626645295517\n",
    "# Best val acc: 0.16483516483516483, Best val loss: 2.3948758443196616, Best train acc: 0.13055555555555556, Best train loss: 2.3958631287450376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam1 = SimpleCNN(device=device,pooling=True)\n",
    "opt_adam1 = torch.optim.Adam(model_adam1.parameters(), lr=LR, weight_decay=0.0005)\n",
    "trainer_m_adam1 = Trainer(model_adam1, criterion, train_loader, val_loader, opt_adam1, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_adam1= trainer_m_adam1.train()\n",
    "# Best val acc: 0.5054945054945055, Best val loss: 2.046719173590342, Best train acc: 0.6888888888888889, Best train loss: 1.8724546173344487\n",
    "# Best val acc: 0.5384615384615384, Best val loss: 1.9763936599095662, Best train acc: 0.7694444444444445, Best train loss: 1.7913675826528799 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam2 = SimpleCNN(device=device,pooling=True)\n",
    "opt_adam2 = torch.optim.Adam(model_adam2.parameters(), lr=LR, weight_decay=0.01)\n",
    "trainer_m_adam2 = Trainer(model_adam2, criterion, train_loader, val_loader, opt_adam2, num_epoch=NUM_EPOCHS, patience=PATIENCE,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_adam2= trainer_m_adam2.train()\n",
    "# Best val acc: 0.5494505494505495, Best val loss: 2.000789761543274, Best train acc: 0.7083333333333334, Best train loss: 1.858591763869576\n",
    "# Best val acc: 0.4725274725274725, Best val loss: 2.0761247873306274, Best train acc: 0.6944444444444444, Best train loss: 1.8865902423858643 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight regularization penalizes large weights to avoid strong dependancy on certain features thus prevent overfitting.<br>\n",
    "Best val acc: 0.5274725274725275, Best val loss: 2.022157371044159, Best train acc: 0.8472222222222222, Best train loss: 1.6982376005338586<br>\n",
    "With SGD as optimizer and weight_decay=0.0005, validation accuracy incerases from 0.516 to 0.527 when compared to no weight decay, therefore it is safe to say that the model generalizes better.<br>\n",
    "<br>\n",
    "Best val acc: 0.5384615384615384, Best val loss: 2.0050004720687866, Best train acc: 0.8944444444444445, Best train loss: 1.6490948096565579<br>\n",
    "With Adam as optimizer and weight_decay=0.005, validation accuracy increases further when compared to SGD, and reaches 0.538. I believe that this improvement is mainly caused by the adaptive learning rate of Adam, yielding a better convergence.<br>\n",
    "<br>\n",
    "Best val acc: 0.5384615384615384, Best val loss: 2.0050004720687866, Best train acc: 0.8944444444444445, Best train loss: 1.6490948096565579 <br>\n",
    "With Adam as optimizer and weight_decay=0.01, validation accuracy<br>\n",
    "<br>\n",
    "Early Stopping:<br>\n",
    "(without weight decay and dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es -> early stopping\n",
    "model_es = SimpleCNN(device=device,pooling=True)\n",
    "opt_es = torch.optim.Adam(model_es.parameters(), lr=LR)\n",
    "trainer_m_es = Trainer(model_es, criterion, train_loader, val_loader, opt_es, num_epoch=NUM_EPOCHS, patience=7,batch_size=BATCH_SIZE,lr_scheduler= None)\n",
    "best_model_es= trainer_m_es.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bla bla bla<br>\n",
    "<br>\n",
    "Learning Rate Scheduling:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr1 = SimpleCNN(device=device,pooling=True)\n",
    "opt_lr1 = torch.optim.Adam(model_lr1.parameters(), lr=LR)\n",
    "scheduler1 = torch.optim.lr_scheduler.StepLR(opt_lr1, step_size=5, gamma=0.1)\n",
    "trainer_m_lr1 = Trainer(model_lr1, criterion, train_loader, val_loader, opt_lr1, num_epoch=NUM_EPOCHS, patience=7,batch_size=BATCH_SIZE,lr_scheduler= scheduler1)\n",
    "best_model_lr1= trainer_m_lr1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr2 = SimpleCNN(device=device,pooling=True)\n",
    "opt_lr2 = torch.optim.Adam(model_lr2.parameters(), lr=LR)\n",
    "scheduler2 =  torch.optim.lr_scheduler.ExponentialLR(opt_lr2, gamma=0.95)\n",
    "trainer_m_lr2 = Trainer(model_lr2, criterion, train_loader, val_loader, opt_lr2, num_epoch=NUM_EPOCHS, patience=7,batch_size=BATCH_SIZE,lr_scheduler= scheduler2)\n",
    "best_model_lr2= trainer_m_lr2.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.5: Experiment with Your Architecture\n",
    "\n",
    "All those parameters at the top of `SimpleCNN` still need to be set. You cannot possibly explore all combinations; so try to change some of them individually to get some feeling for their effect (if any).\n",
    "Optionally, you can explore adding more layers. Report which changes led to the biggest increases and decreases in performance. In particular, what is the effect of making the convolutional layers have (a) a larger filter size, (b) a larger stride and (c) greater depth? How does a pyramidal-shaped network in which the feature maps gradually decrease in height and width but increase in depth compare to a flat architecture, or one with the opposite shape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.6: Optimize Your Architecture\n",
    "Based on your experience with these tests, try to achieve the best performance that you can on the validation set by varying the hyperparameters, architecture, and regularization methods. You can even (optionally) try to think of additional ways to augment the data, or experiment with techniques like local response normalization layers using `torch.nn.LocalResponseNorm` or weight normalization using the implementation [here](https://pytorch.org/docs/stable/_modules/torch/nn/utils/weight_norm.html#weight_norm). Report the best performance you are able to achieve, and the settings you used to obtain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.7: Test Your Final Architecture on Variations of the Data\n",
    "In PyTorch data augmentation can be done dynamically while loading the data using what they call `transforms`. Note that some of the transforms are already implemented. You can\n",
    "try other transformations, such as the ones shown in Figure 3 and also try different probabilities for these transformations. You may find [this link](https://pytorch.org/vision/stable/transforms.html) helpful. Note that the PyTorch data loader refreshes the\n",
    "data in each epoch and apply different transformations to the different instances.\n",
    "\n",
    "Now that you have optimized your architecture, you are ready to test it on augmented data!\n",
    "Report your performance on each of the transformed datasets. Are you surprised by any of the results?\n",
    "Which transformations is your network most invariant to, and which lead it to be unable to recognize the images? What does that tell you about what features your network has learned to use to recognize artists’ images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Transfer Learning with Deep Network\n",
    "\n",
    "In this part, you will fine-tune AlexNet model pretrained on ImageNet to recognize faces. For the sake of simplicity you may use [the pretrained AlexNet model](https://pytorch.org/hub/pytorch_vision_alexnet/) provided in PyTorch Hub. You will\n",
    "work with a subset of the FaceScrub dataset. The subset of male actors is [here](http://www.cs.toronto.edu/~guerzhoy/321/proj1/subset_actors.txt) and the subset of female actors is [here](http://www.cs.toronto.edu/~guerzhoy/321/proj1/subset_actresses.txt). The dataset consists of URLs of images with faces, as well as the bounding boxes of the faces. The format of the bounding box is as follows (from the FaceScrub `readme.txt` file):\n",
    "\n",
    "` \n",
    "The format is x1,y1,x2,y2, where (x1,y1) is the coordinate of the top-left corner of the bounding box and (x2,y2) is that of the bottom-right corner, with (0,0) as the top-left corner of the image. Assuming the image is represented as a Python NumPy array I, a face\n",
    "in I can be obtained as I[y1:y2, x1:x2].\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find it helpful to use and/or modify [this script](www.cs.toronto.edu/~guerzhoy/321/proj1/get_data.py) for downloading the image data. Note that you should crop out the images of the faces and resize them to appropriate size before proceeding further. Make sure to check the SHA-256 hashes, and make sure to only keep faces for which the hashes match. You should set aside 70 images per faces for the training set, and use the rest for the test and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: Train a Multilayer Perceptron\n",
    "First resize the images to 28 × 28 pixels. Use a fully-connected neural network with a single hidden layer of size 300 units.\n",
    "Below, include the learning curve for the test, training, and validation sets, and the final performance classification on the test set. Include a text description of your system. In particular, describe how you preprocessed the input and initialized the weights, what activation function you used, and what the exact architecture of the network that you selected was. You might get performances close to 80-85% accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: AlexNet as a Fixed Feature Extractor\n",
    "Extract the values of the activations of AlexNet on the face images. Use those as features in order to perform face classification: learn a fully-connected neural network that takes in the activations of the units in the AlexNet layer as inputs, and outputs the name of the person. Below, include a description of the system you built and its performance. It is recommended to start out with only using the `conv4` activations. Using `conv4` is sufficient here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3: Visualize Weights\n",
    "Train two networks the way you did in Part 2.1. Use 300 and 800 hidden units in the hidden layer. Visualize 2 different hidden features (neurons) for each of the two settings, and briefly explain why they are interesting. A sample visualization of a hidden feature is shown below. Note that you probably need to use L2 regularization while training to obtain nice weight visualizations.\n",
    "\n",
    "![](figures/figure2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.4: Finetuning AlexNet\n",
    "Train two networks the way you did in Part 2.1. Use 300 and 800 hidden units in the hidden layer. Visualize 2 different hidden features (neurons) for each of the two settings, and briefly explain why they are interesting. A sample visualization of a hidden feature is shown in Figure 4. Note that you probably need to use L2 regularization while training to obtain nice weight visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.5: Bonus: Gradient Visualization\n",
    "Here, you will use [Utku Ozbulak’s PyTorch CNN Visualizations Library](https://github.com/utkuozbulak/pytorch-cnn-visualizations/) to visualize the important parts of the input image for a particular output class. In particular, just select a specific picture of an actor, and then using your trained network in Part 2.4, perform Gradient visualization with guided backpropagation to understand the prediction for that actor with respect to the input image. Comment on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Turn In\n",
    "You have two options for submission:\n",
    "1) Provide all the relevant answers to questions, images, figures, etc, in this Jupyter notebook, convert the jupyter notebook into a PDF, and upload the PDF.\n",
    "2) Write all the answers to the questions and any relevant figures in a LaTeX report, convert the report to a PDF, and upload a zip file containing both the jupyter notebook and the report. \n",
    "\n",
    "## Grading\n",
    "The assignment will be graded out of `100` points: `0` (no submission), `20` (an attempt at a solution), `40` (a partially correct solution), `60` (a mostly correct solution), `80` (a correct solution), `100` (a particularly creative or insightful solution). The grading depends on both the content and clarity of your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp541",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
